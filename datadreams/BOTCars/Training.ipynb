{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model\n",
    "\n",
    "- Decide which feature to use(just try first)\n",
    "- Split training data to training data set and validation data set\n",
    "- Select serval models to train\n",
    "- Generate submission file (change column name 'date' back to 'day' before submission)\n",
    "\n",
    "\n",
    "### Feature explanation\n",
    "\n",
    "- year, year of data\n",
    "- month, month of year \n",
    "- day, day of month\n",
    "- hour, hour of day from 0 ~ 23\n",
    "- weekday, week of day from 0 to 6, 0 is Mon, 1 is Tue, 2 is Wed etc.\n",
    "- grid_id, grid id defined in grid info\n",
    "- temperture, temperture at given time\n",
    "- rainy, 0 means no rain, 1 means is rainy\n",
    "- holiday, 0 means working day, 1 means holiday\n",
    "- car_number, number of cars in that grid at given time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>weekday</th>\n",
       "      <th>grid_id</th>\n",
       "      <th>temperture</th>\n",
       "      <th>car_number</th>\n",
       "      <th>holiday_0</th>\n",
       "      <th>holiday_1</th>\n",
       "      <th>rainy_0</th>\n",
       "      <th>rainy_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month  day  hour  weekday  grid_id  temperture  car_number  \\\n",
       "0  2017      1    2     9        0        1          13        20.0   \n",
       "1  2017      1    2     9        0        2          13         7.0   \n",
       "2  2017      1    2     9        0        3          13         4.0   \n",
       "3  2017      1    2     9        0        4          13         3.0   \n",
       "4  2017      1    2     9        0        6          13         9.0   \n",
       "\n",
       "   holiday_0  holiday_1  rainy_0  rainy_1  \n",
       "0          0          1        1        0  \n",
       "1          0          1        1        0  \n",
       "2          0          1        1        0  \n",
       "3          0          1        1        0  \n",
       "4          0          1        1        0  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import timeit\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "training_features_filepath = 'data/training.csv'\n",
    "test_features_filepath = 'data/test.csv'\n",
    "submission_filepath = 'data/submit_samples.csv'\n",
    "lgb_submission_filepath = 'data/submission_lgb.csv'\n",
    "xgb_submission_filepath = 'data/submission_xgb.csv'\n",
    "ann_submission_filepath = 'data/submission_ann.csv'\n",
    "final_submission_filepath = 'data/submission_final.csv'\n",
    "\n",
    "training_features_xgb_filepath = 'data/training_xgb.csv'\n",
    "test_features_xgb_filepath = 'data/test_xgb.csv'\n",
    "\n",
    "training_features_lgb_filepath = 'data/training_lgb.csv'\n",
    "test_features_lgb_filepath = 'data/test_lgb.csv'\n",
    "\n",
    "drop_columns = ['car_number','year']\n",
    "\n",
    "one_hot_encoding_columns = ['holiday','rainy']\n",
    "\n",
    "# loading data\n",
    "train = pd.read_csv(training_features_filepath)\n",
    "train = pd.get_dummies(train, columns=one_hot_encoding_columns)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>weekday</th>\n",
       "      <th>grid_id</th>\n",
       "      <th>temperture</th>\n",
       "      <th>car_number</th>\n",
       "      <th>holiday_0</th>\n",
       "      <th>holiday_1</th>\n",
       "      <th>rainy_0</th>\n",
       "      <th>rainy_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month  day  hour  weekday  grid_id  temperture  car_number  \\\n",
       "0  2017      3   13     9        0        1           9           0   \n",
       "1  2017      3   13     9        0        2           9           0   \n",
       "2  2017      3   13     9        0        3           9           0   \n",
       "3  2017      3   13     9        0        4           9           0   \n",
       "4  2017      3   13     9        0        5           9           0   \n",
       "\n",
       "   holiday_0  holiday_1  rainy_0  rainy_1  \n",
       "0          1          0        0        1  \n",
       "1          1          0        0        1  \n",
       "2          1          0        0        1  \n",
       "3          1          0        0        1  \n",
       "4          1          0        0        1  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(test_features_filepath)\n",
    "test = pd.get_dummies(test, columns=one_hot_encoding_columns)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_index_by_month_day_hour(df, month, day, hour):\n",
    "    month = df[df['month'] == month]\n",
    "    day = month[month['day'] == day]\n",
    "    hour = day[day['hour'] == hour]\n",
    "    return hour.index[0]\n",
    "\n",
    "def generate_submission_file(pred, output_filepath):\n",
    "    sample_df = pd.read_csv(submission_filepath)\n",
    "    sample_df['car_number'] = pred\n",
    "    sample_df['car_number'] = sample_df['car_number'].apply(lambda x : np.ceil(x))\n",
    "    sample_df['car_number'] = sample_df['car_number'].astype(int)\n",
    "    sample_df.columns = ['grid_id','day','hour','car_number']\n",
    "    sample_df.to_csv(output_filepath, index=False)\n",
    "\n",
    "def split_train_val_data(index, train):\n",
    "    # split train and val by index\n",
    "    train_ = train.iloc[:index]\n",
    "    val_ = train.iloc[index:-1]\n",
    "    # extract x and y\n",
    "    train_x = train_.drop(columns=drop_columns)\n",
    "    train_y = train_['car_number']\n",
    "    val_x = val_.drop(columns=drop_columns)\n",
    "    val_y = val_['car_number']\n",
    "    return train_x, train_y, val_x, val_y\n",
    "\n",
    "def val_or_generate_submission_file_xgb(param_dict, train, test, index, t='train'):\n",
    "    regressor = xgb.XGBRegressor(**param_dict)        \n",
    "    # only train and validate\n",
    "    if t == 'train':\n",
    "        # train with part of data\n",
    "        train_x, train_y, val_x, val_y = split_train_val_data(index, train)\n",
    "        regressor.fit(train_x.as_matrix(), train_y)\n",
    "        pred = regressor.predict(val_x.as_matrix())\n",
    "        print(\"score is \", np.sqrt(mean_squared_error(val_y, pred)))\n",
    "    # using predition as a extra feature\n",
    "    elif t == 'extract':\n",
    "        train_x = train.drop(columns=drop_columns)\n",
    "        train_y = train['car_number']\n",
    "        regressor.fit(train_x.as_matrix(), train_y)\n",
    "        test_ = test.drop(columns=drop_columns)\n",
    "        pred = regressor.predict(test_.as_matrix())\n",
    "        # extract new feature to test\n",
    "        test['xgb'] = np.ceil(pred)\n",
    "        test.to_csv(test_features_xgb_filepath, index=False)\n",
    "        # extract new feature to train\n",
    "        pred = regressor.predict(train_x.as_matrix())\n",
    "        train['xbg'] = np.ceil(pred)\n",
    "        train.to_csv(training_features_xgb_filepath, index=False)        \n",
    "    # using the whole data set to train and do prediction\n",
    "    else:\n",
    "        print(\"Train with all the data and generate submission file\")\n",
    "        # train with all the data\n",
    "        train_x = train.drop(columns=drop_columns)\n",
    "        train_y = train['car_number']\n",
    "        regressor.fit(train_x.as_matrix(), train_y)\n",
    "        test_ = test.drop(columns=drop_columns)\n",
    "        pred = regressor.predict(test_.as_matrix())\n",
    "        generate_submission_file(pred, xgb_submission_filepath)\n",
    "        \n",
    "        \n",
    "def val_or_generate_submission_file_lgbm(param_dict, n_estimators, train, test, index, t='train'):\n",
    "    # only train and validate\n",
    "    if t == 'train':\n",
    "        # train with part of data\n",
    "        train_x, train_y, val_x, val_y = split_train_val_data(index, train)\n",
    "        d_train = lgb.Dataset(train_x, label=train_y)\n",
    "        d_valid = lgb.Dataset(val_x, label=val_y)\n",
    "        watchlist = [d_valid]    \n",
    "        model = lgb.train(param_dict, d_train, n_estimators, watchlist, verbose_eval=0)\n",
    "        pred = model.predict(val_x)\n",
    "        print(\"score is \", np.sqrt(mean_squared_error(val_y, pred)))\n",
    "    # using predition as a extra feature\n",
    "    elif t == 'extract':\n",
    "        train_x = train.drop(columns=drop_columns)\n",
    "        train_y = train['car_number']\n",
    "        d_train = lgb.Dataset(train_x, label=train_y)\n",
    "        model = lgb.train(param_dict, d_train, n_estimators, verbose_eval=0)\n",
    "        test_ = test.drop(columns=drop_columns)\n",
    "        # extract predition to test\n",
    "        pred = model.predict(test_)\n",
    "        test['lgb'] = np.ceil(pred)\n",
    "        test.to_csv(test_features_lgb_filepath, index=False)\n",
    "        # extract prediction to train\n",
    "        pred = model.predict(train_x)\n",
    "        train['lgb'] = np.ceil(pred)\n",
    "        train.to_csv(training_features_lgb_filepath, index=False)        \n",
    "    # using the whole data set to train and do prediction\n",
    "    else:\n",
    "        print(\"Train with all the data and generate submission file\")\n",
    "        # train with all the data\n",
    "        train_x = train.drop(columns=drop_columns)\n",
    "        train_y = train['car_number']\n",
    "        d_train = lgb.Dataset(train_x, label=train_y)\n",
    "        model = lgb.train(param_dict, d_train, n_estimators, verbose_eval=0)\n",
    "        test_ = test.drop(columns=drop_columns)\n",
    "        pred = model.predict(test_)\n",
    "        generate_submission_file(pred, lgb_submission_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODOs\n",
    "\n",
    "- Try use ceil instead of round. **Done test score : 8.0536**\n",
    "- Try remove wind and visibitly and make a submission. **Done test score : 8.0266**\n",
    "- Add average speed as grid feature and make a submission **Done test socre: 8.058**\n",
    "- Add missing data and set car number to average via ceil **Done test socre: 8.0373**\n",
    "- Add missing data and set car number to average via round **Done test socre: 8.0484**\n",
    "- Try LGB and make a submission **Done test socre: 8.0091**\n",
    "- Try filter invalid date in each csv and make a submission via LGB **Done test socre: 8.0012**\n",
    "- Adding avg,std,median car number to features and make a submission via LGB **Done test socre: 8.1684**\n",
    "- Remove outliers and make a submission via LGB **Done test socre: 8.0288**\n",
    "- Try NN or LSTM and make a submission (2018-09-xx)\n",
    "- Try ensemble learning and stacking models (2018-09-xx)\n",
    "- Plot 50 grids on map and see if can find more features for grid (2018-09-xx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB Model 1 - Extract feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict = {\n",
    "    'max_depth' : 5,\n",
    "    'learning_rate' : 0.01,\n",
    "    'n_estimators' : 1500,\n",
    "    'silent' : True,\n",
    "    'objective' : 'reg:linear',\n",
    "    'booster' : 'gbtree',\n",
    "    'n_jobs' : 8,\n",
    "    'gamma' : 0,\n",
    "    'min_child_weight' : 1,\n",
    "    'max_delta_step' : 0,\n",
    "    'subsample' : 0.75,\n",
    "    'colsample_bytree' : 0.9,\n",
    "    'reg_alpha' : 0.7,\n",
    "    'reg_lambda' : 1\n",
    "}\n",
    "index = retrieve_index_by_month_day_hour(train, 3, 6, 9)\n",
    "val_or_generate_submission_file_xgb(param_dict, train, test, index, 'extract')\n",
    "#val_or_generate_submission_file_xgb(param_dict, train, test, index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Light GBM Model 1 - Extract feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 0.1,\n",
    "    'n_jobs': 8,\n",
    "    'metric': 'rmse',\n",
    "    'max_depth': 5,\n",
    "    'learning_rate': 0.01,\n",
    "    'verbose': 0}\n",
    "\n",
    "n_estimators = 4750\n",
    "# reloading data\n",
    "train = pd.read_csv(training_features_xgb_filepath)\n",
    "test = pd.read_csv(test_features_xgb_filepath)\n",
    "index = retrieve_index_by_month_day_hour(train, 3, 6, 9)\n",
    "val_or_generate_submission_file_lgbm(params, n_estimators, train, test, index, 'extract')\n",
    "#val_or_generate_submission_file_lgbm(params, n_estimators, train, test, index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB Model 2 - Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train with all the data and generate submission file\n"
     ]
    }
   ],
   "source": [
    "param_dict = {\n",
    "    'max_depth' : 9,\n",
    "    'learning_rate' : 0.01,\n",
    "    'n_estimators' : 1500,\n",
    "    'silent' : True,\n",
    "    'objective' : 'reg:linear',\n",
    "    'booster' : 'gbtree',\n",
    "    'n_jobs' : 8,\n",
    "    'gamma' : 0,\n",
    "    'min_child_weight' : 1,\n",
    "    'max_delta_step' : 0,\n",
    "    'subsample' : 0.75,\n",
    "    'colsample_bytree' : 0.9,\n",
    "    'reg_alpha' : 0.7,\n",
    "    'reg_lambda' : 1\n",
    "}\n",
    "# loading train and test set with xgb and lgb features\n",
    "train = pd.read_csv(training_features_lgb_filepath)\n",
    "test = pd.read_csv(test_features_lgb_filepath)\n",
    "# predict\n",
    "index = retrieve_index_by_month_day_hour(train, 3, 6, 9)\n",
    "val_or_generate_submission_file_xgb(param_dict, train, test, index, '') # 3.369559474786608 3.362073701927338"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Light GBM Model 2 - Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train with all the data and generate submission file\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 0.1,\n",
    "    'n_jobs': 8,\n",
    "    'metric': 'rmse',\n",
    "    'max_depth': 7,\n",
    "    'learning_rate': 0.01,\n",
    "    'verbose': 0}\n",
    "\n",
    "n_estimators = 4750\n",
    "# loading train and test set with xgb and lgb features\n",
    "train = pd.read_csv(training_features_lgb_filepath)\n",
    "test = pd.read_csv(test_features_lgb_filepath)\n",
    "# prediction\n",
    "index = retrieve_index_by_month_day_hour(train, 3, 6, 9)\n",
    "val_or_generate_submission_file_lgbm(params, n_estimators, train, test, index, '') # 3.3073688776246732 3.289658603021144"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_79 (Dense)             (None, 30)                390       \n",
      "_________________________________________________________________\n",
      "batch_normalization_41 (Batc (None, 30)                120       \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 30)                930       \n",
      "_________________________________________________________________\n",
      "batch_normalization_42 (Batc (None, 30)                120       \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (None, 5)                 155       \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 1,721\n",
      "Trainable params: 1,601\n",
      "Non-trainable params: 120\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# loading train and test data set\n",
    "train = pd.read_csv(training_features_lgb_filepath)\n",
    "test = pd.read_csv(test_features_lgb_filepath)\n",
    "\n",
    "X = train.drop(columns=['year','car_number'])\n",
    "y = train['car_number']\n",
    "x_test = test.drop(columns=['year','car_number'])\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(30, input_shape=(12, ), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(5, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34791 samples, validate on 8698 samples\n",
      "Epoch 1/30\n",
      "34791/34791 [==============================] - 1s 35us/step - loss: 184.2737 - val_loss: 158.6704\n",
      "Epoch 2/30\n",
      "34791/34791 [==============================] - 0s 6us/step - loss: 144.5074 - val_loss: 117.1188\n",
      "Epoch 3/30\n",
      "34791/34791 [==============================] - 0s 6us/step - loss: 107.6268 - val_loss: 86.4108\n",
      "Epoch 4/30\n",
      "34791/34791 [==============================] - 0s 6us/step - loss: 75.1277 - val_loss: 58.0098\n",
      "Epoch 5/30\n",
      "34791/34791 [==============================] - 0s 6us/step - loss: 49.4000 - val_loss: 32.5066\n",
      "Epoch 6/30\n",
      "34791/34791 [==============================] - 0s 6us/step - loss: 32.0397 - val_loss: 21.2759\n",
      "Epoch 7/30\n",
      "34791/34791 [==============================] - 0s 6us/step - loss: 22.0515 - val_loss: 18.3110\n",
      "Epoch 8/30\n",
      "34791/34791 [==============================] - 0s 6us/step - loss: 16.7515 - val_loss: 16.3628\n",
      "Epoch 9/30\n",
      "34791/34791 [==============================] - 0s 6us/step - loss: 13.6401 - val_loss: 14.6710\n",
      "Epoch 10/30\n",
      "34791/34791 [==============================] - 0s 6us/step - loss: 11.9110 - val_loss: 14.1152\n",
      "Epoch 11/30\n",
      "34791/34791 [==============================] - 0s 6us/step - loss: 10.9777 - val_loss: 13.6936\n",
      "Epoch 12/30\n",
      "34791/34791 [==============================] - 0s 6us/step - loss: 10.4451 - val_loss: 13.3187\n",
      "Epoch 13/30\n",
      "34791/34791 [==============================] - 0s 6us/step - loss: 10.2126 - val_loss: 11.9956\n",
      "Epoch 14/30\n",
      "34791/34791 [==============================] - 0s 6us/step - loss: 9.9756 - val_loss: 10.9581\n",
      "Epoch 15/30\n",
      "34791/34791 [==============================] - 0s 6us/step - loss: 10.1209 - val_loss: 10.4767\n",
      "Epoch 16/30\n",
      "34791/34791 [==============================] - 0s 6us/step - loss: 10.0121 - val_loss: 10.5101\n",
      "Epoch 17/30\n",
      "34791/34791 [==============================] - 0s 6us/step - loss: 10.0171 - val_loss: 10.1859\n",
      "Epoch 18/30\n",
      "34791/34791 [==============================] - 0s 6us/step - loss: 9.8065 - val_loss: 9.9994\n",
      "Epoch 19/30\n",
      "34791/34791 [==============================] - 0s 6us/step - loss: 9.9096 - val_loss: 10.0668\n",
      "Epoch 20/30\n",
      "34791/34791 [==============================] - 0s 6us/step - loss: 9.9204 - val_loss: 9.9726\n",
      "Epoch 21/30\n",
      "34791/34791 [==============================] - 0s 6us/step - loss: 9.7849 - val_loss: 9.8872\n",
      "Epoch 22/30\n",
      "34791/34791 [==============================] - 0s 6us/step - loss: 9.7967 - val_loss: 9.8913\n",
      "Epoch 23/30\n",
      "34791/34791 [==============================] - 0s 6us/step - loss: 9.7944 - val_loss: 9.9837\n",
      "Epoch 24/30\n",
      "34791/34791 [==============================] - 0s 6us/step - loss: 9.8534 - val_loss: 9.8919\n",
      "Epoch 25/30\n",
      "34791/34791 [==============================] - 0s 6us/step - loss: 9.8296 - val_loss: 9.7944\n",
      "Epoch 26/30\n",
      "34791/34791 [==============================] - 0s 6us/step - loss: 9.8837 - val_loss: 9.9434\n",
      "Epoch 27/30\n",
      "34791/34791 [==============================] - 0s 6us/step - loss: 9.9131 - val_loss: 9.8287\n",
      "Epoch 28/30\n",
      "34791/34791 [==============================] - 0s 6us/step - loss: 9.6976 - val_loss: 9.9874\n",
      "Epoch 29/30\n",
      "34791/34791 [==============================] - 0s 6us/step - loss: 9.8077 - val_loss: 9.8306\n",
      "Epoch 30/30\n",
      "34791/34791 [==============================] - 0s 6us/step - loss: 9.7625 - val_loss: 9.7847\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2217cc4f748>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit data\n",
    "model.fit(X_train, y_train, epochs=30, batch_size=1024, validation_data=(X_val, y_val), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 43489 samples, validate on 8698 samples\n",
      "Epoch 1/50\n",
      "43489/43489 [==============================] - 1s 31us/step - loss: 192.6048 - val_loss: 157.7018\n",
      "Epoch 2/50\n",
      "43489/43489 [==============================] - 0s 6us/step - loss: 133.4445 - val_loss: 84.4511\n",
      "Epoch 3/50\n",
      "43489/43489 [==============================] - 0s 6us/step - loss: 79.8430 - val_loss: 54.5924\n",
      "Epoch 4/50\n",
      "43489/43489 [==============================] - 0s 6us/step - loss: 39.7945 - val_loss: 32.0508\n",
      "Epoch 5/50\n",
      "43489/43489 [==============================] - 0s 6us/step - loss: 22.8979 - val_loss: 19.8420\n",
      "Epoch 6/50\n",
      "43489/43489 [==============================] - 0s 6us/step - loss: 17.2333 - val_loss: 15.7128\n",
      "Epoch 7/50\n",
      "43489/43489 [==============================] - 0s 6us/step - loss: 14.8062 - val_loss: 13.8418\n",
      "Epoch 8/50\n",
      "43489/43489 [==============================] - 0s 6us/step - loss: 13.6225 - val_loss: 13.2720\n",
      "Epoch 9/50\n",
      "43489/43489 [==============================] - 0s 6us/step - loss: 12.6046 - val_loss: 12.6209\n",
      "Epoch 10/50\n",
      "43489/43489 [==============================] - 0s 6us/step - loss: 11.6789 - val_loss: 11.7584\n",
      "Epoch 11/50\n",
      "43489/43489 [==============================] - 0s 6us/step - loss: 10.8604 - val_loss: 11.3177\n",
      "Epoch 12/50\n",
      "43489/43489 [==============================] - 0s 6us/step - loss: 10.4273 - val_loss: 10.6550\n",
      "Epoch 13/50\n",
      "43489/43489 [==============================] - 0s 6us/step - loss: 10.2498 - val_loss: 10.9969\n",
      "Epoch 14/50\n",
      "43489/43489 [==============================] - 0s 6us/step - loss: 10.0372 - val_loss: 10.3905\n",
      "Epoch 15/50\n",
      "43489/43489 [==============================] - 0s 6us/step - loss: 9.9351 - val_loss: 10.0169\n",
      "Epoch 16/50\n",
      "43489/43489 [==============================] - 0s 6us/step - loss: 9.9264 - val_loss: 9.9969\n",
      "Epoch 17/50\n",
      "43489/43489 [==============================] - 0s 6us/step - loss: 10.0248 - val_loss: 10.6165\n",
      "Epoch 18/50\n",
      "43489/43489 [==============================] - 0s 6us/step - loss: 9.8994 - val_loss: 10.2229\n",
      "Epoch 19/50\n",
      "43489/43489 [==============================] - 0s 6us/step - loss: 9.8990 - val_loss: 9.8236\n",
      "Epoch 20/50\n",
      "43489/43489 [==============================] - 0s 6us/step - loss: 9.8702 - val_loss: 9.9809\n",
      "Epoch 21/50\n",
      "43489/43489 [==============================] - 0s 6us/step - loss: 9.7454 - val_loss: 10.2497\n",
      "Epoch 22/50\n",
      "43489/43489 [==============================] - 0s 6us/step - loss: 9.7523 - val_loss: 9.7634\n",
      "Epoch 23/50\n",
      "43489/43489 [==============================] - 0s 6us/step - loss: 9.8025 - val_loss: 10.2134\n",
      "Epoch 24/50\n",
      "43489/43489 [==============================] - 0s 6us/step - loss: 9.7107 - val_loss: 9.8377\n",
      "Epoch 25/50\n",
      "43489/43489 [==============================] - 0s 6us/step - loss: 9.8031 - val_loss: 9.7272\n",
      "Epoch 26/50\n",
      "43489/43489 [==============================] - 0s 6us/step - loss: 9.8240 - val_loss: 9.6709\n",
      "Epoch 27/50\n",
      "43489/43489 [==============================] - 0s 6us/step - loss: 9.7903 - val_loss: 9.6661\n",
      "Epoch 28/50\n",
      "43489/43489 [==============================] - 0s 6us/step - loss: 9.7750 - val_loss: 9.8386\n",
      "Epoch 29/50\n",
      "43489/43489 [==============================] - 0s 6us/step - loss: 9.7005 - val_loss: 9.6734\n",
      "Epoch 30/50\n",
      "43489/43489 [==============================] - 0s 6us/step - loss: 9.6967 - val_loss: 9.7901\n",
      "Epoch 31/50\n",
      "43489/43489 [==============================] - 0s 6us/step - loss: 9.7418 - val_loss: 9.6835\n",
      "Epoch 32/50\n",
      "43489/43489 [==============================] - 0s 6us/step - loss: 9.7170 - val_loss: 9.7004\n",
      "Epoch 33/50\n",
      "43489/43489 [==============================] - 0s 6us/step - loss: 9.6983 - val_loss: 9.6536\n",
      "Epoch 34/50\n",
      "43489/43489 [==============================] - 0s 6us/step - loss: 9.7408 - val_loss: 9.7612\n",
      "Epoch 35/50\n",
      "43489/43489 [==============================] - 0s 6us/step - loss: 9.6916 - val_loss: 9.6747\n",
      "Epoch 36/50\n",
      "43489/43489 [==============================] - 0s 6us/step - loss: 9.6777 - val_loss: 10.1132\n",
      "Epoch 37/50\n",
      "43489/43489 [==============================] - 0s 6us/step - loss: 9.7203 - val_loss: 9.6355\n",
      "Epoch 38/50\n",
      "43489/43489 [==============================] - 0s 6us/step - loss: 9.7196 - val_loss: 9.8053\n",
      "Epoch 39/50\n",
      "43489/43489 [==============================] - 0s 6us/step - loss: 9.7007 - val_loss: 9.6371\n",
      "Epoch 40/50\n",
      "43489/43489 [==============================] - 0s 6us/step - loss: 9.6886 - val_loss: 9.6589\n",
      "Epoch 41/50\n",
      "43489/43489 [==============================] - 0s 6us/step - loss: 9.6745 - val_loss: 9.8687\n",
      "Epoch 42/50\n",
      "43489/43489 [==============================] - 0s 6us/step - loss: 9.6752 - val_loss: 9.7335\n",
      "Epoch 43/50\n",
      "43489/43489 [==============================] - 0s 6us/step - loss: 9.7290 - val_loss: 9.7205\n",
      "Epoch 44/50\n",
      "43489/43489 [==============================] - 0s 6us/step - loss: 9.6422 - val_loss: 9.6914\n",
      "Epoch 45/50\n",
      "43489/43489 [==============================] - 0s 6us/step - loss: 9.6596 - val_loss: 9.6832\n",
      "Epoch 46/50\n",
      "43489/43489 [==============================] - 0s 6us/step - loss: 9.6660 - val_loss: 9.7770\n",
      "Epoch 47/50\n",
      "43489/43489 [==============================] - 0s 6us/step - loss: 9.6692 - val_loss: 9.7355\n",
      "Epoch 48/50\n",
      "43489/43489 [==============================] - 0s 6us/step - loss: 9.6832 - val_loss: 9.6434\n",
      "Epoch 49/50\n",
      "43489/43489 [==============================] - 0s 6us/step - loss: 9.6769 - val_loss: 9.6210\n",
      "Epoch 50/50\n",
      "43489/43489 [==============================] - 0s 6us/step - loss: 9.6723 - val_loss: 9.6686\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2217e320cc0>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the whole data set\n",
    "model.fit(X, y, epochs=50, batch_size=1024, validation_data=(X_val, y_val), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predit and generate submission file\n",
    "pred = model.predict(x_test)\n",
    "generate_submission_file(pred, ann_submission_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Averaging prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading prediction file\n",
    "xgb_pred = pd.read_csv(xgb_submission_filepath)\n",
    "lgb_pred = pd.read_csv(lgb_submission_filepath)\n",
    "# rename car_number\n",
    "xgb_pred = xgb_pred.rename(columns={'car_number': 'car_number_xgb'})\n",
    "lgb_pred = lgb_pred.rename(columns={'car_number': 'car_number_lgb'})\n",
    "# merge 2 dataframes\n",
    "avg_sub = pd.merge(xgb_pred, lgb_pred, on=['grid_id','day','hour'], how='left')\n",
    "avg_sub['car_number'] = 0\n",
    "avg_sub['car_number'] = np.ceil((avg_sub.car_number_xgb + avg_sub.car_number_lgb)/2)\n",
    "avg_sub = avg_sub.drop(columns=['car_number_xgb', 'car_number_lgb'])\n",
    "avg_sub.to_csv(final_submission_filepath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_21 (LSTM)               (None, 784, 20)           2320      \n",
      "_________________________________________________________________\n",
      "lstm_22 (LSTM)               (None, 10)                1240      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 3,571\n",
      "Trainable params: 3,571\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "train_grid_id_1 = train[train['grid_id']==1]\n",
    "train_grid_id_1 = train_grid_id_1.reset_index()\n",
    "index = retrieve_index_by_month_day_hour(train_grid_id_1, 3, 6, 9)\n",
    "train_x, train_y, val_x, val_y = split_train_val_data(index, train_grid_id_1)\n",
    "\n",
    "train_x = train_x.drop(columns=['grid_id'])\n",
    "val_x = val_x.drop(columns=['grid_id'])\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "train_x = min_max_scaler.fit_transform(train_x)\n",
    "val_x = min_max_scaler.fit_transform(val_x)\n",
    "\n",
    "train_x = np.reshape(train_x, (1, 784, 8))\n",
    "val_x = np.reshape(val_x, (1, 97, 8))\n",
    "\n",
    "lstm = Sequential()\n",
    "lstm.add(LSTM(20, dropout=0.1, recurrent_dropout=0.1, input_shape=(784, 8), return_sequences=True))\n",
    "lstm.add(LSTM(10, dropout=0.1, recurrent_dropout=0.1))\n",
    "lstm.add(Dense(1))\n",
    "lstm.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "lstm.summary()\n",
    "\n",
    "#lstm.fit(train_x, train_y, epochs=100, batch_size=1, validation_data=(val_x, val_y), verbose=1)\n",
    "#score, mse = lstm.evaluate(val_x, val_y, batch_size=1)\n",
    "#print(score + \", \" + mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
