{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model\n",
    "\n",
    "- Decide which feature to use(just try first)\n",
    "- Split training data to training data set and validation data set\n",
    "- Select serval models to train\n",
    "- Generate submission file (change column name 'date' back to 'day' before submission)\n",
    "\n",
    "\n",
    "### Feature explanation\n",
    "\n",
    "- year, year of data\n",
    "- month, month of year \n",
    "- day, day of month\n",
    "- hour, hour of day from 0 ~ 23\n",
    "- weekday, week of day from 0 to 6, 0 is Mon, 1 is Tue, 2 is Wed etc.\n",
    "- grid_id, grid id defined in grid info\n",
    "- temperture, temperture at given time\n",
    "- rainy, 0 means no rain, 1 means is rainy\n",
    "- holiday, 0 means working day, 1 means holiday\n",
    "- car_number, number of cars in that grid at given time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>weekday</th>\n",
       "      <th>grid_id</th>\n",
       "      <th>temperture</th>\n",
       "      <th>rainy</th>\n",
       "      <th>holiday</th>\n",
       "      <th>car_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month  day  hour  weekday  grid_id  temperture  rainy  holiday  \\\n",
       "0  2017      1    2     9        0        1          13      0        1   \n",
       "1  2017      1    2     9        0        2          13      0        1   \n",
       "2  2017      1    2     9        0        3          13      0        1   \n",
       "3  2017      1    2     9        0        4          13      0        1   \n",
       "4  2017      1    2     9        0        6          13      0        1   \n",
       "\n",
       "   car_number  \n",
       "0        20.0  \n",
       "1         7.0  \n",
       "2         4.0  \n",
       "3         3.0  \n",
       "4         9.0  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import timeit\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "training_features_filepath = 'data/training.csv'\n",
    "test_features_filepath = 'data/test.csv'\n",
    "submission_filepath = 'data/submit_samples.csv'\n",
    "lgb_submission_filepath = 'data/submission_lgb.csv'\n",
    "xgb_submission_filepath = 'data/submission_xgb.csv'\n",
    "ann_submission_filepath = 'data/submission_ann.csv'\n",
    "final_submission_filepath = 'data/submission_final.csv'\n",
    "\n",
    "training_features_xgb_filepath = 'data/training_xgb.csv'\n",
    "test_features_xgb_filepath = 'data/test_xgb.csv'\n",
    "\n",
    "training_features_lgb_filepath = 'data/training_lgb.csv'\n",
    "test_features_lgb_filepath = 'data/test_lgb.csv'\n",
    "\n",
    "drop_columns = ['car_number','year']\n",
    "\n",
    "one_hot_encoding_columns = ['weekday','grid_id','holiday','rainy']\n",
    "\n",
    "# loading data\n",
    "train = pd.read_csv(training_features_filepath)\n",
    "#train = pd.get_dummies(train, columns=one_hot_encoding_columns)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>weekday</th>\n",
       "      <th>grid_id</th>\n",
       "      <th>temperture</th>\n",
       "      <th>rainy</th>\n",
       "      <th>holiday</th>\n",
       "      <th>car_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month  day  hour  weekday  grid_id  temperture  rainy  holiday  \\\n",
       "0  2017      3   13     9        0        1           9      1        0   \n",
       "1  2017      3   13     9        0        2           9      1        0   \n",
       "2  2017      3   13     9        0        3           9      1        0   \n",
       "3  2017      3   13     9        0        4           9      1        0   \n",
       "4  2017      3   13     9        0        5           9      1        0   \n",
       "\n",
       "   car_number  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(test_features_filepath)\n",
    "#test = pd.get_dummies(test, columns=one_hot_encoding_columns)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_index_by_month_day_hour(df, month, day, hour):\n",
    "    month = df[df['month'] == month]\n",
    "    day = month[month['day'] == day]\n",
    "    hour = day[day['hour'] == hour]\n",
    "    return hour.index[0]\n",
    "\n",
    "def generate_submission_file(pred, output_filepath):\n",
    "    sample_df = pd.read_csv(submission_filepath)\n",
    "    sample_df['car_number'] = pred\n",
    "    sample_df['car_number'] = sample_df['car_number'].apply(lambda x : np.ceil(x))\n",
    "    sample_df['car_number'] = sample_df['car_number'].astype(int)\n",
    "    sample_df.columns = ['grid_id','day','hour','car_number']\n",
    "    sample_df.to_csv(output_filepath, index=False)\n",
    "\n",
    "def split_train_val_data(index, train):\n",
    "    # split train and val by index\n",
    "    train_ = train.iloc[:index]\n",
    "    val_ = train.iloc[index:-1]\n",
    "    # extract x and y\n",
    "    train_x = train_.drop(columns=drop_columns)\n",
    "    train_y = train_['car_number']\n",
    "    val_x = val_.drop(columns=drop_columns)\n",
    "    val_y = val_['car_number']\n",
    "    return train_x, train_y, val_x, val_y\n",
    "\n",
    "def val_or_generate_submission_file_xgb(param_dict, train, test, index, t='train'):\n",
    "    regressor = xgb.XGBRegressor(**param_dict)        \n",
    "    # only train and validate\n",
    "    if t == 'train':\n",
    "        # train with part of data\n",
    "        train_x, train_y, val_x, val_y = split_train_val_data(index, train)\n",
    "        regressor.fit(train_x.as_matrix(), train_y)\n",
    "        pred = regressor.predict(val_x.as_matrix())\n",
    "        print(\"score is \", np.sqrt(mean_squared_error(val_y, pred)))\n",
    "    # using predition as a extra feature\n",
    "    elif t == 'extract':\n",
    "        train_x = train.drop(columns=drop_columns)\n",
    "        train_y = train['car_number']\n",
    "        regressor.fit(train_x.as_matrix(), train_y)\n",
    "        test_ = test.drop(columns=drop_columns)\n",
    "        pred = regressor.predict(test_.as_matrix())\n",
    "        # extract new feature to test\n",
    "        test['xgb'] = np.ceil(pred)\n",
    "        test.to_csv(test_features_xgb_filepath, index=False)\n",
    "        # extract new feature to train\n",
    "        pred = regressor.predict(train_x.as_matrix())\n",
    "        train['xbg'] = np.ceil(pred)\n",
    "        train.to_csv(training_features_xgb_filepath, index=False)        \n",
    "    # using the whole data set to train and do prediction\n",
    "    else:\n",
    "        print(\"Train with all the data and generate submission file\")\n",
    "        # train with all the data\n",
    "        train_x = train.drop(columns=drop_columns)\n",
    "        train_y = train['car_number']\n",
    "        regressor.fit(train_x.as_matrix(), train_y)\n",
    "        test_ = test.drop(columns=drop_columns)\n",
    "        pred = regressor.predict(test_.as_matrix())\n",
    "        generate_submission_file(pred, xgb_submission_filepath)\n",
    "        \n",
    "        \n",
    "def val_or_generate_submission_file_lgbm(param_dict, n_estimators, train, test, index, t='train'):\n",
    "    # only train and validate\n",
    "    if t == 'train':\n",
    "        # train with part of data\n",
    "        train_x, train_y, val_x, val_y = split_train_val_data(index, train)\n",
    "        d_train = lgb.Dataset(train_x, label=train_y)\n",
    "        d_valid = lgb.Dataset(val_x, label=val_y)\n",
    "        watchlist = [d_valid]    \n",
    "        model = lgb.train(param_dict, d_train, n_estimators, watchlist, verbose_eval=0)\n",
    "        pred = model.predict(val_x)\n",
    "        print(\"score is \", np.sqrt(mean_squared_error(val_y, pred)))\n",
    "    # using predition as a extra feature\n",
    "    elif t == 'extract':\n",
    "        train_x = train.drop(columns=drop_columns)\n",
    "        train_y = train['car_number']\n",
    "        d_train = lgb.Dataset(train_x, label=train_y)\n",
    "        model = lgb.train(param_dict, d_train, n_estimators, verbose_eval=0)\n",
    "        test_ = test.drop(columns=drop_columns)\n",
    "        # extract predition to test\n",
    "        pred = model.predict(test_)\n",
    "        test['lgb'] = np.ceil(pred)\n",
    "        test.to_csv(test_features_lgb_filepath, index=False)\n",
    "        # extract prediction to train\n",
    "        pred = model.predict(train_x)\n",
    "        train['lgb'] = np.ceil(pred)\n",
    "        train.to_csv(training_features_lgb_filepath, index=False)        \n",
    "    # using the whole data set to train and do prediction\n",
    "    else:\n",
    "        print(\"Train with all the data and generate submission file\")\n",
    "        # train with all the data\n",
    "        train_x = train.drop(columns=drop_columns)\n",
    "        train_y = train['car_number']\n",
    "        d_train = lgb.Dataset(train_x, label=train_y)\n",
    "        model = lgb.train(param_dict, d_train, n_estimators, verbose_eval=0)\n",
    "        test_ = test.drop(columns=drop_columns)\n",
    "        pred = model.predict(test_)\n",
    "        generate_submission_file(pred, lgb_submission_filepath)\n",
    "        \n",
    "def plot_model(history):\n",
    "    train_loss = history.history['loss']\n",
    "    #train_acc = history.history['acc']\n",
    "    val_loss = history.history['val_loss']\n",
    "    #val_acc = history.history['val_acc']\n",
    "\n",
    "    niter = np.arange(len(train_loss))\n",
    "\n",
    "    #fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    \n",
    "    fig, (ax1) = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "    ax1.plot(niter, train_loss, label='train')\n",
    "    ax1.plot(niter, val_loss, label='validation')\n",
    "    ax1.set_title(\"loss\")\n",
    "    ax1.set_xlabel('ecphos')\n",
    "    ax1.set_ylabel('loss')\n",
    "    #ax1.set_ylim(0, 100)\n",
    "\n",
    "    #ax2.plot(niter, train_acc, label='train')\n",
    "    #ax2.plot(niter, val_acc, label='validation')\n",
    "    #ax2.set_title(\"accuracy\")\n",
    "    #ax2.set_xlabel('ecphos')\n",
    "    #ax2.set_ylabel('accuracy')\n",
    "    #ax2.set_ylim(0.85, 1)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODOs\n",
    "\n",
    "- Try use ceil instead of round. **Done test score : 8.0536**\n",
    "- Try remove wind and visibitly and make a submission. **Done test score : 8.0266**\n",
    "- Add average speed as grid feature and make a submission **Done test socre: 8.058**\n",
    "- Add missing data and set car number to average via ceil **Done test socre: 8.0373**\n",
    "- Add missing data and set car number to average via round **Done test socre: 8.0484**\n",
    "- Try LGB and make a submission **Done test socre: 8.0091**\n",
    "- Try filter invalid date in each csv and make a submission via LGB **Done test socre: 8.0012**\n",
    "- Adding avg,std,median car number to features and make a submission via LGB **Done test socre: 8.1684**\n",
    "- Remove outliers and make a submission via LGB **Done test socre: 8.0288**\n",
    "- Try NN or LSTM and make a submission (2018-09-xx)\n",
    "- Try ensemble learning and stacking models (2018-09-xx)\n",
    "- Plot 50 grids on map and see if can find more features for grid (2018-09-xx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB Model 1 - Extract feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict = {\n",
    "    'max_depth' : 5,\n",
    "    'learning_rate' : 0.01,\n",
    "    'n_estimators' : 1500,\n",
    "    'silent' : True,\n",
    "    'objective' : 'reg:linear',\n",
    "    'booster' : 'gbtree',\n",
    "    'n_jobs' : 8,\n",
    "    'gamma' : 0,\n",
    "    'min_child_weight' : 1,\n",
    "    'max_delta_step' : 0,\n",
    "    'subsample' : 0.75,\n",
    "    'colsample_bytree' : 0.9,\n",
    "    'reg_alpha' : 0.7,\n",
    "    'reg_lambda' : 1\n",
    "}\n",
    "index = retrieve_index_by_month_day_hour(train, 3, 6, 9)\n",
    "val_or_generate_submission_file_xgb(param_dict, train, test, index, 'extract')\n",
    "#val_or_generate_submission_file_xgb(param_dict, train, test, index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Light GBM Model 1 - Extract feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 0.1,\n",
    "    'n_jobs': 8,\n",
    "    'metric': 'rmse',\n",
    "    'max_depth': 5,\n",
    "    'learning_rate': 0.01,\n",
    "    'verbose': 0}\n",
    "\n",
    "n_estimators = 4750\n",
    "# reloading data\n",
    "train = pd.read_csv(training_features_xgb_filepath)\n",
    "test = pd.read_csv(test_features_xgb_filepath)\n",
    "index = retrieve_index_by_month_day_hour(train, 3, 6, 9)\n",
    "val_or_generate_submission_file_lgbm(params, n_estimators, train, test, index, 'extract')\n",
    "#val_or_generate_submission_file_lgbm(params, n_estimators, train, test, index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB Model 2 - Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train with all the data and generate submission file\n"
     ]
    }
   ],
   "source": [
    "param_dict = {\n",
    "    'max_depth' : 9,\n",
    "    'learning_rate' : 0.01,\n",
    "    'n_estimators' : 1500,\n",
    "    'silent' : True,\n",
    "    'objective' : 'reg:linear',\n",
    "    'booster' : 'gbtree',\n",
    "    'n_jobs' : 8,\n",
    "    'gamma' : 0,\n",
    "    'min_child_weight' : 1,\n",
    "    'max_delta_step' : 0,\n",
    "    'subsample' : 0.75,\n",
    "    'colsample_bytree' : 0.9,\n",
    "    'reg_alpha' : 0.7,\n",
    "    'reg_lambda' : 1\n",
    "}\n",
    "# loading train and test set with xgb and lgb features\n",
    "#train = pd.read_csv(training_features_lgb_filepath)\n",
    "#test = pd.read_csv(test_features_lgb_filepath)\n",
    "# predict\n",
    "index = retrieve_index_by_month_day_hour(train, 3, 6, 9)\n",
    "val_or_generate_submission_file_xgb(param_dict, train, test, index,'') # 3.369559474786608 3.362073701927338"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Light GBM Model 2 - Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train with all the data and generate submission file\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 0.1,\n",
    "    'n_jobs': 8,\n",
    "    'metric': 'rmse',\n",
    "    'max_depth': 7,\n",
    "    'learning_rate': 0.01,\n",
    "    'verbose': 0}\n",
    "\n",
    "n_estimators = 4750\n",
    "# loading train and test set with xgb and lgb features\n",
    "#train = pd.read_csv(training_features_lgb_filepath)\n",
    "#test = pd.read_csv(test_features_lgb_filepath)\n",
    "# prediction\n",
    "index = retrieve_index_by_month_day_hour(train, 3, 6, 9)\n",
    "val_or_generate_submission_file_lgbm(params, n_estimators, train, test, index,'') # 3.3073688776246732 3.289658603021144"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_50 (Dense)             (None, 256)               16896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 316,929\n",
      "Trainable params: 314,881\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Embedding\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import optimizers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# loading train and test data set\n",
    "train = pd.read_csv(training_features_filepath)\n",
    "test = pd.read_csv(test_features_filepath)\n",
    "\n",
    "# retrieve length of train data for later use\n",
    "train_idx = train.shape[0]\n",
    "\n",
    "# retrieve target\n",
    "train_y = train['car_number']\n",
    "\n",
    "# drop uncessary columns\n",
    "train = train.drop(columns=drop_columns)\n",
    "test = test.drop(columns=drop_columns)\n",
    "\n",
    "# merging train and test\n",
    "data = train.append(test)\n",
    "\n",
    "# one hot encoding\n",
    "data = pd.get_dummies(data, columns=one_hot_encoding_columns)\n",
    "\n",
    "# normalization\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(data)\n",
    "data = scaler.transform(data)\n",
    "\n",
    "# split train and test\n",
    "train_x = data[:train_idx]\n",
    "test_x = data[train_idx:-1]\n",
    "\n",
    "# adam = optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "#model.add(Embedding(train_x.shape[1] + 1, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(Dense(256, input_shape=(train_x.shape[1],), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 38654 samples, validate on 4834 samples\n",
      "Epoch 1/500\n",
      "38654/38654 [==============================] - 5s 126us/step - loss: 186.0808 - val_loss: 222.3218\n",
      "Epoch 2/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 168.7250 - val_loss: 209.4016\n",
      "Epoch 3/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 151.1661 - val_loss: 186.1636\n",
      "Epoch 4/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 134.5292 - val_loss: 157.0854\n",
      "Epoch 5/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 118.3973 - val_loss: 133.1351\n",
      "Epoch 6/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 103.4770 - val_loss: 117.0067\n",
      "Epoch 7/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 92.0962 - val_loss: 104.7356\n",
      "Epoch 8/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 83.2154 - val_loss: 94.9523\n",
      "Epoch 9/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 76.4276 - val_loss: 87.2639\n",
      "Epoch 10/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 71.2263 - val_loss: 81.3703\n",
      "Epoch 11/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 67.5727 - val_loss: 77.0266\n",
      "Epoch 12/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 64.9832 - val_loss: 74.0858\n",
      "Epoch 13/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 63.2369 - val_loss: 72.0145\n",
      "Epoch 14/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 61.9249 - val_loss: 70.6831\n",
      "Epoch 15/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 60.9801 - val_loss: 69.6305\n",
      "Epoch 16/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 60.4536 - val_loss: 68.9848\n",
      "Epoch 17/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 60.0693 - val_loss: 68.4626\n",
      "Epoch 18/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 59.7176 - val_loss: 68.0768\n",
      "Epoch 19/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 59.4642 - val_loss: 67.8007\n",
      "Epoch 20/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 59.3977 - val_loss: 67.5375\n",
      "Epoch 21/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 59.2557 - val_loss: 67.3457\n",
      "Epoch 22/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 59.1366 - val_loss: 67.1563\n",
      "Epoch 23/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 58.9288 - val_loss: 67.0688\n",
      "Epoch 24/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 58.8740 - val_loss: 66.8976\n",
      "Epoch 25/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 58.7070 - val_loss: 66.6947\n",
      "Epoch 26/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 58.6711 - val_loss: 66.6093\n",
      "Epoch 27/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 58.5400 - val_loss: 66.5276\n",
      "Epoch 28/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 58.4695 - val_loss: 66.5331\n",
      "Epoch 29/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 58.2812 - val_loss: 66.4713\n",
      "Epoch 30/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 58.2173 - val_loss: 66.1640\n",
      "Epoch 31/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 58.1694 - val_loss: 66.1354\n",
      "Epoch 32/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 58.0662 - val_loss: 65.9612\n",
      "Epoch 33/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 58.0198 - val_loss: 65.9458\n",
      "Epoch 34/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 57.8187 - val_loss: 65.7694\n",
      "Epoch 35/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 57.8211 - val_loss: 65.6777\n",
      "Epoch 36/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 57.7102 - val_loss: 65.5745\n",
      "Epoch 37/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 57.6743 - val_loss: 65.4810\n",
      "Epoch 38/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 57.6322 - val_loss: 65.4008\n",
      "Epoch 39/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 57.4476 - val_loss: 65.3648\n",
      "Epoch 40/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 57.4223 - val_loss: 65.2633\n",
      "Epoch 41/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 57.3095 - val_loss: 65.0951\n",
      "Epoch 42/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 57.3067 - val_loss: 65.0784\n",
      "Epoch 43/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 57.1141 - val_loss: 65.1507\n",
      "Epoch 44/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 56.9935 - val_loss: 64.9597\n",
      "Epoch 45/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 56.9028 - val_loss: 64.8754\n",
      "Epoch 46/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 56.9123 - val_loss: 64.6248\n",
      "Epoch 47/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 56.8431 - val_loss: 64.7715\n",
      "Epoch 48/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 56.6858 - val_loss: 64.5771\n",
      "Epoch 49/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 56.8136 - val_loss: 64.4644\n",
      "Epoch 50/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 56.5785 - val_loss: 64.3319\n",
      "Epoch 51/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 56.4421 - val_loss: 64.3573\n",
      "Epoch 52/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 56.4251 - val_loss: 64.1314\n",
      "Epoch 53/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 56.2533 - val_loss: 64.1483\n",
      "Epoch 54/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 56.2493 - val_loss: 64.3251\n",
      "Epoch 55/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 56.1823 - val_loss: 64.1940\n",
      "Epoch 56/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 55.9847 - val_loss: 64.3054\n",
      "Epoch 57/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 56.0029 - val_loss: 63.9311\n",
      "Epoch 58/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 55.8174 - val_loss: 64.1003\n",
      "Epoch 59/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 55.7688 - val_loss: 63.8224\n",
      "Epoch 60/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 55.6823 - val_loss: 63.8511\n",
      "Epoch 61/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 55.5828 - val_loss: 63.8060\n",
      "Epoch 62/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 55.8205 - val_loss: 63.3177\n",
      "Epoch 63/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 55.6295 - val_loss: 63.3952\n",
      "Epoch 64/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 55.4960 - val_loss: 63.4044\n",
      "Epoch 65/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 55.4043 - val_loss: 63.4097\n",
      "Epoch 66/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 55.3043 - val_loss: 63.1818\n",
      "Epoch 67/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 55.2235 - val_loss: 63.1347\n",
      "Epoch 68/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 55.2533 - val_loss: 63.0729\n",
      "Epoch 69/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 55.1188 - val_loss: 62.9250\n",
      "Epoch 70/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 55.1851 - val_loss: 62.7729\n",
      "Epoch 71/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 55.0343 - val_loss: 62.7526\n",
      "Epoch 72/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 54.9441 - val_loss: 62.9547\n",
      "Epoch 73/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 54.7793 - val_loss: 62.7628\n",
      "Epoch 74/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 54.7135 - val_loss: 62.3790\n",
      "Epoch 75/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 54.7409 - val_loss: 62.6745\n",
      "Epoch 76/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 54.4596 - val_loss: 62.5622\n",
      "Epoch 77/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 54.6718 - val_loss: 62.4375\n",
      "Epoch 78/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 54.5075 - val_loss: 61.9967\n",
      "Epoch 79/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 54.4049 - val_loss: 62.1859\n",
      "Epoch 80/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 54.3053 - val_loss: 62.3956\n",
      "Epoch 81/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 54.3630 - val_loss: 61.8266\n",
      "Epoch 82/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 54.1589 - val_loss: 61.9430\n",
      "Epoch 83/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 54.0323 - val_loss: 62.1873\n",
      "Epoch 84/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 53.9892 - val_loss: 61.5699\n",
      "Epoch 85/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 54.0342 - val_loss: 61.6461\n",
      "Epoch 86/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 53.9055 - val_loss: 61.6636\n",
      "Epoch 87/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 53.7903 - val_loss: 61.5987\n",
      "Epoch 88/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 53.8373 - val_loss: 61.4595\n",
      "Epoch 89/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 53.8295 - val_loss: 61.4333\n",
      "Epoch 90/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 53.5609 - val_loss: 61.6009\n",
      "Epoch 91/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 53.5520 - val_loss: 61.2604\n",
      "Epoch 92/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 53.5735 - val_loss: 61.2134\n",
      "Epoch 93/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 53.3974 - val_loss: 61.1116\n",
      "Epoch 94/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 53.3655 - val_loss: 61.0831\n",
      "Epoch 95/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 53.3493 - val_loss: 61.0123\n",
      "Epoch 96/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 53.2802 - val_loss: 61.4058\n",
      "Epoch 97/500\n",
      "38654/38654 [==============================] - 0s 11us/step - loss: 53.1817 - val_loss: 60.9553\n",
      "Epoch 98/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 53.0429 - val_loss: 60.6487\n",
      "Epoch 99/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 53.0696 - val_loss: 60.6026\n",
      "Epoch 100/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 52.9332 - val_loss: 60.3247\n",
      "Epoch 101/500\n",
      "38654/38654 [==============================] - 0s 11us/step - loss: 52.9239 - val_loss: 60.4521\n",
      "Epoch 102/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 52.8547 - val_loss: 60.3223\n",
      "Epoch 103/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 52.7796 - val_loss: 60.2923\n",
      "Epoch 104/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 52.6551 - val_loss: 60.8639\n",
      "Epoch 105/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 52.6225 - val_loss: 60.3527\n",
      "Epoch 106/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 52.6201 - val_loss: 60.0295\n",
      "Epoch 107/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 52.5343 - val_loss: 60.3764\n",
      "Epoch 108/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 52.3369 - val_loss: 59.9058\n",
      "Epoch 109/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 52.2857 - val_loss: 59.6917\n",
      "Epoch 110/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 52.5412 - val_loss: 59.8463\n",
      "Epoch 111/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 52.2984 - val_loss: 60.1468\n",
      "Epoch 112/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 52.1679 - val_loss: 59.7124\n",
      "Epoch 113/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 52.2847 - val_loss: 59.4590\n",
      "Epoch 114/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 52.2889 - val_loss: 60.4798\n",
      "Epoch 115/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 52.2872 - val_loss: 59.8445\n",
      "Epoch 116/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 52.1328 - val_loss: 59.7491\n",
      "Epoch 117/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 51.7603 - val_loss: 59.5964\n",
      "Epoch 118/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 52.0553 - val_loss: 59.6208\n",
      "Epoch 119/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 51.9260 - val_loss: 59.4915\n",
      "Epoch 120/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 51.7764 - val_loss: 59.1437\n",
      "Epoch 121/500\n",
      "38654/38654 [==============================] - 0s 11us/step - loss: 51.8138 - val_loss: 59.2702\n",
      "Epoch 122/500\n",
      "38654/38654 [==============================] - 0s 11us/step - loss: 51.7383 - val_loss: 59.0483\n",
      "Epoch 123/500\n",
      "38654/38654 [==============================] - 0s 12us/step - loss: 51.5579 - val_loss: 59.0662\n",
      "Epoch 124/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 51.6426 - val_loss: 58.9634\n",
      "Epoch 125/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 51.8119 - val_loss: 59.0515\n",
      "Epoch 126/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 51.6358 - val_loss: 58.9449\n",
      "Epoch 127/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 51.6283 - val_loss: 59.3934\n",
      "Epoch 128/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 51.5728 - val_loss: 58.7784\n",
      "Epoch 129/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 51.5699 - val_loss: 59.0709\n",
      "Epoch 130/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 51.4264 - val_loss: 58.7523\n",
      "Epoch 131/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 51.3851 - val_loss: 58.6759\n",
      "Epoch 132/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 51.4946 - val_loss: 58.8188\n",
      "Epoch 133/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 51.1569 - val_loss: 58.6109\n",
      "Epoch 134/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 51.4204 - val_loss: 58.6279\n",
      "Epoch 135/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 51.3483 - val_loss: 58.7702\n",
      "Epoch 136/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 51.2988 - val_loss: 58.6097\n",
      "Epoch 137/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 51.1517 - val_loss: 59.0798\n",
      "Epoch 138/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 51.2876 - val_loss: 58.5133\n",
      "Epoch 139/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 51.0693 - val_loss: 58.4038\n",
      "Epoch 140/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 51.1554 - val_loss: 58.3553\n",
      "Epoch 141/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 51.1786 - val_loss: 58.3308\n",
      "Epoch 142/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 51.0824 - val_loss: 58.7381\n",
      "Epoch 143/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 51.0274 - val_loss: 58.5444\n",
      "Epoch 144/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 50.7694 - val_loss: 58.4738\n",
      "Epoch 145/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 51.0059 - val_loss: 58.2057\n",
      "Epoch 146/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 50.9321 - val_loss: 58.3380\n",
      "Epoch 147/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 50.8308 - val_loss: 58.3758\n",
      "Epoch 148/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 50.8846 - val_loss: 58.1157\n",
      "Epoch 149/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 50.8674 - val_loss: 58.1615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 51.0023 - val_loss: 58.8624\n",
      "Epoch 151/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 50.6849 - val_loss: 58.5410\n",
      "Epoch 152/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 50.9545 - val_loss: 58.2034\n",
      "Epoch 153/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 50.7819 - val_loss: 58.0467\n",
      "Epoch 154/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 50.5361 - val_loss: 58.0706\n",
      "Epoch 155/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 50.6498 - val_loss: 58.4515\n",
      "Epoch 156/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 50.6721 - val_loss: 58.2393\n",
      "Epoch 157/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 50.6564 - val_loss: 58.0964\n",
      "Epoch 158/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 50.6511 - val_loss: 57.9524\n",
      "Epoch 159/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 50.6930 - val_loss: 58.1703\n",
      "Epoch 160/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 50.6580 - val_loss: 58.0804\n",
      "Epoch 161/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 50.6417 - val_loss: 57.8354\n",
      "Epoch 162/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 50.6345 - val_loss: 57.9012\n",
      "Epoch 163/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 50.7077 - val_loss: 58.1616\n",
      "Epoch 164/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 50.4727 - val_loss: 57.8858\n",
      "Epoch 165/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 50.7441 - val_loss: 57.9108\n",
      "Epoch 166/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 50.4520 - val_loss: 58.6334\n",
      "Epoch 167/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 50.7131 - val_loss: 57.9118\n",
      "Epoch 168/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 50.4763 - val_loss: 57.9145\n",
      "Epoch 169/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 50.4368 - val_loss: 57.9359\n",
      "Epoch 170/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 50.4228 - val_loss: 57.9631\n",
      "Epoch 171/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 50.4014 - val_loss: 57.9407\n",
      "Epoch 172/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 50.6413 - val_loss: 57.7981\n",
      "Epoch 173/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 50.4425 - val_loss: 57.7223\n",
      "Epoch 174/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 50.4880 - val_loss: 57.8282\n",
      "Epoch 175/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 50.3657 - val_loss: 57.7667\n",
      "Epoch 176/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 50.2120 - val_loss: 57.7443\n",
      "Epoch 177/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 50.3704 - val_loss: 57.7684\n",
      "Epoch 178/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 50.5039 - val_loss: 57.8207\n",
      "Epoch 179/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 50.1915 - val_loss: 57.5600\n",
      "Epoch 180/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 50.5508 - val_loss: 57.8575\n",
      "Epoch 181/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 50.3567 - val_loss: 57.3429\n",
      "Epoch 182/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 50.2062 - val_loss: 57.8124\n",
      "Epoch 183/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 50.0481 - val_loss: 57.7070\n",
      "Epoch 184/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 50.1633 - val_loss: 57.5336\n",
      "Epoch 185/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 50.2358 - val_loss: 57.5473\n",
      "Epoch 186/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 50.2877 - val_loss: 57.4624\n",
      "Epoch 187/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 50.3056 - val_loss: 57.7653\n",
      "Epoch 188/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 50.2285 - val_loss: 57.6899\n",
      "Epoch 189/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 50.1513 - val_loss: 57.5000\n",
      "Epoch 190/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 50.2851 - val_loss: 57.6389\n",
      "Epoch 191/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 50.0127 - val_loss: 57.5873\n",
      "Epoch 192/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 50.2077 - val_loss: 57.3288\n",
      "Epoch 193/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 50.1173 - val_loss: 57.7098\n",
      "Epoch 194/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 50.0469 - val_loss: 57.6149\n",
      "Epoch 195/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 50.0417 - val_loss: 57.5547\n",
      "Epoch 196/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 50.1311 - val_loss: 57.3002\n",
      "Epoch 197/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 50.0446 - val_loss: 57.4280\n",
      "Epoch 198/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 50.0279 - val_loss: 57.6317\n",
      "Epoch 199/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 50.2221 - val_loss: 57.1126\n",
      "Epoch 200/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 50.1245 - val_loss: 57.2995\n",
      "Epoch 201/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 50.0447 - val_loss: 57.3537\n",
      "Epoch 202/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.9665 - val_loss: 57.3915\n",
      "Epoch 203/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 50.1496 - val_loss: 57.1909\n",
      "Epoch 204/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 50.0693 - val_loss: 57.1637\n",
      "Epoch 205/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 50.1262 - val_loss: 57.4398\n",
      "Epoch 206/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 50.1845 - val_loss: 57.1048\n",
      "Epoch 207/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 50.1046 - val_loss: 57.2802\n",
      "Epoch 208/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 50.0505 - val_loss: 57.3240\n",
      "Epoch 209/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.7673 - val_loss: 57.7220\n",
      "Epoch 210/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.8079 - val_loss: 57.1118\n",
      "Epoch 211/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.9668 - val_loss: 57.1196\n",
      "Epoch 212/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 50.0091 - val_loss: 57.4091\n",
      "Epoch 213/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.7817 - val_loss: 57.1794\n",
      "Epoch 214/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.9508 - val_loss: 57.5102\n",
      "Epoch 215/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.8527 - val_loss: 57.1867\n",
      "Epoch 216/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.8291 - val_loss: 57.2168\n",
      "Epoch 217/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.9453 - val_loss: 57.2429\n",
      "Epoch 218/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.7819 - val_loss: 57.6955\n",
      "Epoch 219/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 50.1906 - val_loss: 57.2959\n",
      "Epoch 220/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 50.0195 - val_loss: 57.4574\n",
      "Epoch 221/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.8632 - val_loss: 57.0424\n",
      "Epoch 222/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.7222 - val_loss: 57.1983\n",
      "Epoch 223/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 50.0036 - val_loss: 57.0390\n",
      "Epoch 224/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.9056 - val_loss: 56.7013\n",
      "Epoch 225/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.9078 - val_loss: 57.4155\n",
      "Epoch 226/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.8119 - val_loss: 57.1689\n",
      "Epoch 227/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 50.0528 - val_loss: 57.2417\n",
      "Epoch 228/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.8326 - val_loss: 57.0033\n",
      "Epoch 229/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.8746 - val_loss: 57.3539\n",
      "Epoch 230/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.7132 - val_loss: 56.8311\n",
      "Epoch 231/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.7547 - val_loss: 57.1576\n",
      "Epoch 232/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.7854 - val_loss: 57.0867\n",
      "Epoch 233/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.7497 - val_loss: 57.2161\n",
      "Epoch 234/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.5290 - val_loss: 57.0863\n",
      "Epoch 235/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 50.0517 - val_loss: 56.8239\n",
      "Epoch 236/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.9242 - val_loss: 56.7576\n",
      "Epoch 237/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.7062 - val_loss: 57.4152\n",
      "Epoch 238/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.6511 - val_loss: 57.9598\n",
      "Epoch 239/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.7765 - val_loss: 56.8098\n",
      "Epoch 240/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.4654 - val_loss: 56.8401\n",
      "Epoch 241/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.5796 - val_loss: 56.7911\n",
      "Epoch 242/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.7572 - val_loss: 57.2096\n",
      "Epoch 243/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.5486 - val_loss: 56.6988\n",
      "Epoch 244/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.6549 - val_loss: 56.5641\n",
      "Epoch 245/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.6759 - val_loss: 57.0608\n",
      "Epoch 246/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.7880 - val_loss: 57.0037\n",
      "Epoch 247/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.4595 - val_loss: 56.4796\n",
      "Epoch 248/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.6356 - val_loss: 56.8710\n",
      "Epoch 249/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.7708 - val_loss: 57.5211\n",
      "Epoch 250/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.3572 - val_loss: 56.9317\n",
      "Epoch 251/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.5653 - val_loss: 56.9262\n",
      "Epoch 252/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.3655 - val_loss: 56.8111\n",
      "Epoch 253/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.6401 - val_loss: 56.7361\n",
      "Epoch 254/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.5776 - val_loss: 57.1251\n",
      "Epoch 255/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.4305 - val_loss: 56.7864\n",
      "Epoch 256/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.5749 - val_loss: 57.0313\n",
      "Epoch 257/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.7053 - val_loss: 56.6193\n",
      "Epoch 258/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.2963 - val_loss: 56.4630\n",
      "Epoch 259/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.7831 - val_loss: 56.6050\n",
      "Epoch 260/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.4932 - val_loss: 57.3203\n",
      "Epoch 261/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.5685 - val_loss: 57.2959\n",
      "Epoch 262/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.5008 - val_loss: 56.6441\n",
      "Epoch 263/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.5167 - val_loss: 56.3523\n",
      "Epoch 264/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.5545 - val_loss: 56.5208\n",
      "Epoch 265/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.3612 - val_loss: 56.7639\n",
      "Epoch 266/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.3979 - val_loss: 56.7106\n",
      "Epoch 267/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.5123 - val_loss: 56.4990\n",
      "Epoch 268/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.6009 - val_loss: 56.3714\n",
      "Epoch 269/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.6596 - val_loss: 56.8210\n",
      "Epoch 270/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.3612 - val_loss: 56.5678\n",
      "Epoch 271/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.7190 - val_loss: 56.4743\n",
      "Epoch 272/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.5663 - val_loss: 56.5385\n",
      "Epoch 273/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.2679 - val_loss: 56.5905\n",
      "Epoch 274/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.4814 - val_loss: 56.5921\n",
      "Epoch 275/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.4107 - val_loss: 56.3168\n",
      "Epoch 276/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.5979 - val_loss: 56.5028\n",
      "Epoch 277/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.6237 - val_loss: 56.5410\n",
      "Epoch 278/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.4549 - val_loss: 56.3388\n",
      "Epoch 279/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.3826 - val_loss: 56.5898\n",
      "Epoch 280/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.3447 - val_loss: 56.8309\n",
      "Epoch 281/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.4459 - val_loss: 56.4028\n",
      "Epoch 282/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.3374 - val_loss: 56.3307\n",
      "Epoch 283/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.3373 - val_loss: 56.4246\n",
      "Epoch 284/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.3459 - val_loss: 56.4759\n",
      "Epoch 285/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.4397 - val_loss: 56.2195\n",
      "Epoch 286/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.5088 - val_loss: 56.4593\n",
      "Epoch 287/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.5182 - val_loss: 56.5258\n",
      "Epoch 288/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.1862 - val_loss: 56.9336\n",
      "Epoch 289/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.3900 - val_loss: 56.1941\n",
      "Epoch 290/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.3841 - val_loss: 56.2517\n",
      "Epoch 291/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.2926 - val_loss: 56.3248\n",
      "Epoch 292/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.2181 - val_loss: 56.6083\n",
      "Epoch 293/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.3292 - val_loss: 56.2688\n",
      "Epoch 294/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.2725 - val_loss: 56.3860\n",
      "Epoch 295/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.1153 - val_loss: 56.6840\n",
      "Epoch 296/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.2732 - val_loss: 56.5496\n",
      "Epoch 297/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.3359 - val_loss: 56.7326\n",
      "Epoch 298/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.5348 - val_loss: 56.3880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.0459 - val_loss: 56.3816\n",
      "Epoch 300/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.1490 - val_loss: 56.4597\n",
      "Epoch 301/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.2208 - val_loss: 56.3481\n",
      "Epoch 302/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.2769 - val_loss: 56.3070\n",
      "Epoch 303/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.0339 - val_loss: 57.0967\n",
      "Epoch 304/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.0961 - val_loss: 56.4147\n",
      "Epoch 305/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.2846 - val_loss: 56.4735\n",
      "Epoch 306/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.2636 - val_loss: 56.3528\n",
      "Epoch 307/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.1285 - val_loss: 56.6174\n",
      "Epoch 308/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.2127 - val_loss: 56.5950\n",
      "Epoch 309/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.3500 - val_loss: 56.7473\n",
      "Epoch 310/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.0749 - val_loss: 56.7319\n",
      "Epoch 311/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.3421 - val_loss: 57.0079\n",
      "Epoch 312/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.1539 - val_loss: 56.3969\n",
      "Epoch 313/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.9896 - val_loss: 56.4981\n",
      "Epoch 314/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.1849 - val_loss: 56.9222\n",
      "Epoch 315/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.1076 - val_loss: 57.0538\n",
      "Epoch 316/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.1267 - val_loss: 56.4064\n",
      "Epoch 317/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.1488 - val_loss: 56.4683\n",
      "Epoch 318/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.0052 - val_loss: 56.3247\n",
      "Epoch 319/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.3731 - val_loss: 57.3596\n",
      "Epoch 320/500\n",
      "38654/38654 [==============================] - 0s 11us/step - loss: 49.1206 - val_loss: 56.4552\n",
      "Epoch 321/500\n",
      "38654/38654 [==============================] - 0s 11us/step - loss: 49.3564 - val_loss: 56.5731\n",
      "Epoch 322/500\n",
      "38654/38654 [==============================] - 0s 11us/step - loss: 49.0799 - val_loss: 56.2983\n",
      "Epoch 323/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.0586 - val_loss: 56.5020\n",
      "Epoch 324/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.9481 - val_loss: 56.4473\n",
      "Epoch 325/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.2051 - val_loss: 56.4659\n",
      "Epoch 326/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.0478 - val_loss: 57.3189\n",
      "Epoch 327/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.2740 - val_loss: 56.5169\n",
      "Epoch 328/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.1870 - val_loss: 56.5751\n",
      "Epoch 329/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.8906 - val_loss: 56.5181\n",
      "Epoch 330/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.0581 - val_loss: 56.7132\n",
      "Epoch 331/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.8803 - val_loss: 56.4548\n",
      "Epoch 332/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.0755 - val_loss: 56.3831\n",
      "Epoch 333/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.2448 - val_loss: 56.6492\n",
      "Epoch 334/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.0120 - val_loss: 56.6447\n",
      "Epoch 335/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.9540 - val_loss: 56.5073\n",
      "Epoch 336/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.9187 - val_loss: 56.5135\n",
      "Epoch 337/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.8507 - val_loss: 57.0623\n",
      "Epoch 338/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.9971 - val_loss: 56.3639\n",
      "Epoch 339/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.7690 - val_loss: 56.7500\n",
      "Epoch 340/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.8548 - val_loss: 56.7587\n",
      "Epoch 341/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.1172 - val_loss: 56.6344\n",
      "Epoch 342/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.9711 - val_loss: 56.5235\n",
      "Epoch 343/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.1108 - val_loss: 56.3599\n",
      "Epoch 344/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.0077 - val_loss: 56.6305\n",
      "Epoch 345/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.0181 - val_loss: 57.5138\n",
      "Epoch 346/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.8760 - val_loss: 57.3060\n",
      "Epoch 347/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.8647 - val_loss: 56.5871\n",
      "Epoch 348/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.0711 - val_loss: 56.8314\n",
      "Epoch 349/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.9685 - val_loss: 56.5745\n",
      "Epoch 350/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.7067 - val_loss: 56.3930\n",
      "Epoch 351/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.6887 - val_loss: 57.2035\n",
      "Epoch 352/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.1209 - val_loss: 56.3909\n",
      "Epoch 353/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.8241 - val_loss: 56.7533\n",
      "Epoch 354/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.9354 - val_loss: 56.4255\n",
      "Epoch 355/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.9214 - val_loss: 56.7490\n",
      "Epoch 356/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.2967 - val_loss: 57.2649\n",
      "Epoch 357/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.8013 - val_loss: 56.8288\n",
      "Epoch 358/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.0069 - val_loss: 56.9613\n",
      "Epoch 359/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.8091 - val_loss: 57.6728\n",
      "Epoch 360/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.7830 - val_loss: 56.9145\n",
      "Epoch 361/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.7220 - val_loss: 57.2052\n",
      "Epoch 362/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.0389 - val_loss: 56.3195\n",
      "Epoch 363/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.8667 - val_loss: 56.6596\n",
      "Epoch 364/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 49.2617 - val_loss: 56.3780\n",
      "Epoch 365/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.6127 - val_loss: 56.6659\n",
      "Epoch 366/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.7609 - val_loss: 56.6613\n",
      "Epoch 367/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.8293 - val_loss: 56.8700\n",
      "Epoch 368/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.5323 - val_loss: 56.7330\n",
      "Epoch 369/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.8285 - val_loss: 57.5812\n",
      "Epoch 370/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.6626 - val_loss: 56.5027\n",
      "Epoch 371/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.8440 - val_loss: 57.4057\n",
      "Epoch 372/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.6912 - val_loss: 57.7164\n",
      "Epoch 373/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.5756 - val_loss: 56.8687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 374/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.4764 - val_loss: 57.0488\n",
      "Epoch 375/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.7065 - val_loss: 56.7651\n",
      "Epoch 376/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.5856 - val_loss: 56.6583\n",
      "Epoch 377/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.9281 - val_loss: 58.0840\n",
      "Epoch 378/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 49.0592 - val_loss: 57.3477\n",
      "Epoch 379/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.7045 - val_loss: 56.5522\n",
      "Epoch 380/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.5170 - val_loss: 56.5770\n",
      "Epoch 381/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.7658 - val_loss: 57.2235\n",
      "Epoch 382/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.4349 - val_loss: 56.5390\n",
      "Epoch 383/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.5567 - val_loss: 56.3280\n",
      "Epoch 384/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.7631 - val_loss: 58.2868\n",
      "Epoch 385/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.5655 - val_loss: 56.9667\n",
      "Epoch 386/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.6138 - val_loss: 58.4905\n",
      "Epoch 387/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.7075 - val_loss: 57.4774\n",
      "Epoch 388/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.5572 - val_loss: 57.7374\n",
      "Epoch 389/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.6514 - val_loss: 56.9874\n",
      "Epoch 390/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.6931 - val_loss: 56.6740\n",
      "Epoch 391/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.5690 - val_loss: 56.8238\n",
      "Epoch 392/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.7117 - val_loss: 56.9130\n",
      "Epoch 393/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.4484 - val_loss: 57.7628\n",
      "Epoch 394/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.6602 - val_loss: 57.8332\n",
      "Epoch 395/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.5999 - val_loss: 57.1897\n",
      "Epoch 396/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.6752 - val_loss: 56.9301\n",
      "Epoch 397/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.6171 - val_loss: 57.7211\n",
      "Epoch 398/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.4743 - val_loss: 57.6020\n",
      "Epoch 399/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.9131 - val_loss: 57.8889\n",
      "Epoch 400/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.6261 - val_loss: 57.0991\n",
      "Epoch 401/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.5962 - val_loss: 57.5079\n",
      "Epoch 402/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.6899 - val_loss: 56.8929\n",
      "Epoch 403/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.6777 - val_loss: 57.2172\n",
      "Epoch 404/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.8064 - val_loss: 57.3719\n",
      "Epoch 405/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.5518 - val_loss: 58.6824\n",
      "Epoch 406/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.4624 - val_loss: 56.5664\n",
      "Epoch 407/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.6485 - val_loss: 57.1679\n",
      "Epoch 408/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.5597 - val_loss: 56.8798\n",
      "Epoch 409/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.5665 - val_loss: 57.7224\n",
      "Epoch 410/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.5763 - val_loss: 56.7229\n",
      "Epoch 411/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.6939 - val_loss: 58.6849\n",
      "Epoch 412/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.6568 - val_loss: 57.3007\n",
      "Epoch 413/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.3991 - val_loss: 56.6029\n",
      "Epoch 414/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.7570 - val_loss: 56.9487\n",
      "Epoch 415/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.4772 - val_loss: 57.4125\n",
      "Epoch 416/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.4046 - val_loss: 57.0198\n",
      "Epoch 417/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.7315 - val_loss: 57.8920\n",
      "Epoch 418/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.5398 - val_loss: 57.2056\n",
      "Epoch 419/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.5984 - val_loss: 57.0170\n",
      "Epoch 420/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.3122 - val_loss: 56.7959\n",
      "Epoch 421/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.3757 - val_loss: 58.2667\n",
      "Epoch 422/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.5513 - val_loss: 58.2163\n",
      "Epoch 423/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.6037 - val_loss: 58.1719\n",
      "Epoch 424/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.6438 - val_loss: 57.2768\n",
      "Epoch 425/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.3729 - val_loss: 57.7834\n",
      "Epoch 426/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.4667 - val_loss: 57.1078\n",
      "Epoch 427/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.5889 - val_loss: 57.0238\n",
      "Epoch 428/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.4169 - val_loss: 56.8085\n",
      "Epoch 429/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.2937 - val_loss: 57.7558\n",
      "Epoch 430/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.3191 - val_loss: 57.2658\n",
      "Epoch 431/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.4418 - val_loss: 57.8167\n",
      "Epoch 432/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.4670 - val_loss: 57.4955\n",
      "Epoch 433/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.6207 - val_loss: 58.0116\n",
      "Epoch 434/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.8380 - val_loss: 57.1878\n",
      "Epoch 435/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.4376 - val_loss: 57.6473\n",
      "Epoch 436/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.8007 - val_loss: 57.6463\n",
      "Epoch 437/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.5088 - val_loss: 57.5066\n",
      "Epoch 438/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.3250 - val_loss: 57.8299\n",
      "Epoch 439/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.3875 - val_loss: 57.5490\n",
      "Epoch 440/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.3031 - val_loss: 56.9634\n",
      "Epoch 441/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.3269 - val_loss: 57.7568\n",
      "Epoch 442/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.5369 - val_loss: 60.1119\n",
      "Epoch 443/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.4420 - val_loss: 56.9588\n",
      "Epoch 444/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.5601 - val_loss: 57.8615\n",
      "Epoch 445/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.3699 - val_loss: 56.8922\n",
      "Epoch 446/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.4698 - val_loss: 57.4771\n",
      "Epoch 447/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.4473 - val_loss: 57.3189\n",
      "Epoch 448/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.3695 - val_loss: 57.8352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 449/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.1834 - val_loss: 57.9617\n",
      "Epoch 450/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.4357 - val_loss: 56.6495\n",
      "Epoch 451/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.6077 - val_loss: 57.5011\n",
      "Epoch 452/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.4498 - val_loss: 58.5020\n",
      "Epoch 453/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.2814 - val_loss: 56.9057\n",
      "Epoch 454/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.3235 - val_loss: 58.0737\n",
      "Epoch 455/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.3369 - val_loss: 57.2588\n",
      "Epoch 456/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.4171 - val_loss: 57.9995\n",
      "Epoch 457/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.2770 - val_loss: 57.6253\n",
      "Epoch 458/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.5996 - val_loss: 58.0700\n",
      "Epoch 459/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.2348 - val_loss: 57.7057\n",
      "Epoch 460/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.4729 - val_loss: 57.1703\n",
      "Epoch 461/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.1810 - val_loss: 56.8928\n",
      "Epoch 462/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.0744 - val_loss: 57.6200\n",
      "Epoch 463/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.1810 - val_loss: 57.6867\n",
      "Epoch 464/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.0736 - val_loss: 57.4305\n",
      "Epoch 465/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.3839 - val_loss: 57.1596\n",
      "Epoch 466/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.4335 - val_loss: 58.2507\n",
      "Epoch 467/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.0912 - val_loss: 58.5256\n",
      "Epoch 468/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.2172 - val_loss: 57.3938\n",
      "Epoch 469/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.4177 - val_loss: 57.7238\n",
      "Epoch 470/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.3475 - val_loss: 57.5253\n",
      "Epoch 471/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 47.9924 - val_loss: 57.5387\n",
      "Epoch 472/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.1098 - val_loss: 58.0059\n",
      "Epoch 473/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.2911 - val_loss: 58.0758\n",
      "Epoch 474/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.3142 - val_loss: 56.8037\n",
      "Epoch 475/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.0137 - val_loss: 57.9965\n",
      "Epoch 476/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.2531 - val_loss: 57.8899\n",
      "Epoch 477/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.0832 - val_loss: 57.8237\n",
      "Epoch 478/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.6889 - val_loss: 58.2933\n",
      "Epoch 479/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.2691 - val_loss: 59.1896\n",
      "Epoch 480/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.4931 - val_loss: 56.4628\n",
      "Epoch 481/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.2658 - val_loss: 58.9915\n",
      "Epoch 482/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.4377 - val_loss: 57.0968\n",
      "Epoch 483/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 47.9820 - val_loss: 57.7883\n",
      "Epoch 484/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 47.9058 - val_loss: 58.0208\n",
      "Epoch 485/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.1258 - val_loss: 57.4179\n",
      "Epoch 486/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.1865 - val_loss: 57.1476\n",
      "Epoch 487/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.0229 - val_loss: 57.2465\n",
      "Epoch 488/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 47.9511 - val_loss: 56.9162\n",
      "Epoch 489/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 47.8964 - val_loss: 58.2729\n",
      "Epoch 490/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.2265 - val_loss: 58.1905\n",
      "Epoch 491/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.0357 - val_loss: 57.1471\n",
      "Epoch 492/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.4850 - val_loss: 57.8939\n",
      "Epoch 493/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 47.9937 - val_loss: 57.6654\n",
      "Epoch 494/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 47.9942 - val_loss: 57.1868\n",
      "Epoch 495/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.1873 - val_loss: 58.8566\n",
      "Epoch 496/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.1037 - val_loss: 57.7157\n",
      "Epoch 497/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.0827 - val_loss: 58.3804\n",
      "Epoch 498/500\n",
      "38654/38654 [==============================] - 0s 10us/step - loss: 48.2848 - val_loss: 57.7095\n",
      "Epoch 499/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.1535 - val_loss: 58.2371\n",
      "Epoch 500/500\n",
      "38654/38654 [==============================] - 0s 9us/step - loss: 48.4095 - val_loss: 56.9952\n"
     ]
    }
   ],
   "source": [
    "# fit data\n",
    "history = model.fit(X_train, y_train, epochs=500, batch_size=512, validation_data=(X_val, y_val), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_model(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43489, 65) (43489,)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape, train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "43489/43489 [==============================] - 3s 73us/step - loss: 24.0457\n",
      "Epoch 2/50\n",
      "43489/43489 [==============================] - 2s 45us/step - loss: 14.9626\n",
      "Epoch 3/50\n",
      "43489/43489 [==============================] - 2s 46us/step - loss: 14.1951\n",
      "Epoch 4/50\n",
      "43489/43489 [==============================] - 2s 45us/step - loss: 13.6794\n",
      "Epoch 5/50\n",
      "43489/43489 [==============================] - 2s 45us/step - loss: 13.3169\n",
      "Epoch 6/50\n",
      "43489/43489 [==============================] - 2s 46us/step - loss: 13.0003\n",
      "Epoch 7/50\n",
      "43489/43489 [==============================] - 2s 45us/step - loss: 12.7298\n",
      "Epoch 8/50\n",
      "43489/43489 [==============================] - 2s 46us/step - loss: 12.5208\n",
      "Epoch 9/50\n",
      "43489/43489 [==============================] - 2s 46us/step - loss: 12.3253\n",
      "Epoch 10/50\n",
      "43489/43489 [==============================] - 2s 46us/step - loss: 12.2715\n",
      "Epoch 11/50\n",
      "43489/43489 [==============================] - 2s 47us/step - loss: 12.0261\n",
      "Epoch 12/50\n",
      "43489/43489 [==============================] - 2s 46us/step - loss: 11.9038\n",
      "Epoch 13/50\n",
      "43489/43489 [==============================] - 2s 47us/step - loss: 11.8197\n",
      "Epoch 14/50\n",
      "43489/43489 [==============================] - 2s 47us/step - loss: 11.7019\n",
      "Epoch 15/50\n",
      "43489/43489 [==============================] - 2s 47us/step - loss: 11.6157\n",
      "Epoch 16/50\n",
      "43489/43489 [==============================] - 2s 46us/step - loss: 11.5423\n",
      "Epoch 17/50\n",
      "43489/43489 [==============================] - 2s 47us/step - loss: 11.4589\n",
      "Epoch 18/50\n",
      "43489/43489 [==============================] - 2s 46us/step - loss: 11.3263\n",
      "Epoch 19/50\n",
      "43489/43489 [==============================] - 2s 46us/step - loss: 11.1587\n",
      "Epoch 20/50\n",
      "43489/43489 [==============================] - 2s 46us/step - loss: 11.1038\n",
      "Epoch 21/50\n",
      "43489/43489 [==============================] - 2s 47us/step - loss: 11.0048\n",
      "Epoch 22/50\n",
      "43489/43489 [==============================] - 2s 47us/step - loss: 10.9672\n",
      "Epoch 23/50\n",
      "43489/43489 [==============================] - 2s 46us/step - loss: 10.8393\n",
      "Epoch 24/50\n",
      "43489/43489 [==============================] - 2s 47us/step - loss: 10.7847\n",
      "Epoch 25/50\n",
      "43489/43489 [==============================] - 2s 47us/step - loss: 10.7420\n",
      "Epoch 26/50\n",
      "43489/43489 [==============================] - 2s 46us/step - loss: 10.6765\n",
      "Epoch 27/50\n",
      "43489/43489 [==============================] - 2s 46us/step - loss: 10.6495\n",
      "Epoch 28/50\n",
      "43489/43489 [==============================] - 2s 47us/step - loss: 10.5745\n",
      "Epoch 29/50\n",
      "43489/43489 [==============================] - 2s 46us/step - loss: 10.5230\n",
      "Epoch 30/50\n",
      "43489/43489 [==============================] - 2s 46us/step - loss: 10.4935\n",
      "Epoch 31/50\n",
      "43489/43489 [==============================] - 2s 46us/step - loss: 10.4193\n",
      "Epoch 32/50\n",
      "43489/43489 [==============================] - 2s 46us/step - loss: 10.3360\n",
      "Epoch 33/50\n",
      "43489/43489 [==============================] - 2s 47us/step - loss: 10.3369\n",
      "Epoch 34/50\n",
      "43489/43489 [==============================] - 2s 47us/step - loss: 10.2177\n",
      "Epoch 35/50\n",
      "43489/43489 [==============================] - 2s 46us/step - loss: 10.2003\n",
      "Epoch 36/50\n",
      "43489/43489 [==============================] - 2s 45us/step - loss: 10.1284\n",
      "Epoch 37/50\n",
      "43489/43489 [==============================] - ETA: 0s - loss: 10.11 - 2s 45us/step - loss: 10.1196\n",
      "Epoch 38/50\n",
      "43489/43489 [==============================] - 2s 46us/step - loss: 10.0592\n",
      "Epoch 39/50\n",
      "43489/43489 [==============================] - 2s 46us/step - loss: 9.9912\n",
      "Epoch 40/50\n",
      "43489/43489 [==============================] - 2s 45us/step - loss: 9.8978\n",
      "Epoch 41/50\n",
      "43489/43489 [==============================] - 2s 46us/step - loss: 9.9586\n",
      "Epoch 42/50\n",
      "43489/43489 [==============================] - 2s 47us/step - loss: 9.8285\n",
      "Epoch 43/50\n",
      "43489/43489 [==============================] - 2s 46us/step - loss: 9.8140\n",
      "Epoch 44/50\n",
      "43489/43489 [==============================] - 2s 46us/step - loss: 9.7834\n",
      "Epoch 45/50\n",
      "43489/43489 [==============================] - 2s 47us/step - loss: 9.7782\n",
      "Epoch 46/50\n",
      "43489/43489 [==============================] - 2s 46us/step - loss: 9.7566\n",
      "Epoch 47/50\n",
      "43489/43489 [==============================] - 2s 47us/step - loss: 9.6559\n",
      "Epoch 48/50\n",
      "43489/43489 [==============================] - 2s 47us/step - loss: 9.5894\n",
      "Epoch 49/50\n",
      "43489/43489 [==============================] - 2s 46us/step - loss: 9.5497\n",
      "Epoch 50/50\n",
      "43489/43489 [==============================] - 2s 47us/step - loss: 9.5629\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24ee8a21860>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the whole data set\n",
    "model.fit(train_x, train_y, epochs=50, batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predit and generate submission file\n",
    "pred = model.predict(x_test)\n",
    "generate_submission_file(pred, ann_submission_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Averaging prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading prediction file\n",
    "xgb_pred = pd.read_csv(xgb_submission_filepath)\n",
    "lgb_pred = pd.read_csv(lgb_submission_filepath)\n",
    "# rename car_number\n",
    "xgb_pred = xgb_pred.rename(columns={'car_number': 'car_number_xgb'})\n",
    "lgb_pred = lgb_pred.rename(columns={'car_number': 'car_number_lgb'})\n",
    "# merge 2 dataframes\n",
    "avg_sub = pd.merge(xgb_pred, lgb_pred, on=['grid_id','day','hour'], how='left')\n",
    "avg_sub['car_number'] = 0\n",
    "avg_sub['car_number'] = np.ceil((avg_sub.car_number_xgb + avg_sub.car_number_lgb) * 1.05 /2)\n",
    "avg_sub = avg_sub.drop(columns=['car_number_xgb', 'car_number_lgb'])\n",
    "avg_sub.to_csv(final_submission_filepath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_21 (LSTM)               (None, 784, 20)           2320      \n",
      "_________________________________________________________________\n",
      "lstm_22 (LSTM)               (None, 10)                1240      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 3,571\n",
      "Trainable params: 3,571\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "train_grid_id_1 = train[train['grid_id']==1]\n",
    "train_grid_id_1 = train_grid_id_1.reset_index()\n",
    "index = retrieve_index_by_month_day_hour(train_grid_id_1, 3, 6, 9)\n",
    "train_x, train_y, val_x, val_y = split_train_val_data(index, train_grid_id_1)\n",
    "\n",
    "train_x = train_x.drop(columns=['grid_id'])\n",
    "val_x = val_x.drop(columns=['grid_id'])\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "train_x = min_max_scaler.fit_transform(train_x)\n",
    "val_x = min_max_scaler.fit_transform(val_x)\n",
    "\n",
    "train_x = np.reshape(train_x, (1, 784, 8))\n",
    "val_x = np.reshape(val_x, (1, 97, 8))\n",
    "\n",
    "lstm = Sequential()\n",
    "lstm.add(LSTM(20, dropout=0.1, recurrent_dropout=0.1, input_shape=(784, 8), return_sequences=True))\n",
    "lstm.add(LSTM(10, dropout=0.1, recurrent_dropout=0.1))\n",
    "lstm.add(Dense(1))\n",
    "lstm.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "lstm.summary()\n",
    "\n",
    "#lstm.fit(train_x, train_y, epochs=100, batch_size=1, validation_data=(val_x, val_y), verbose=1)\n",
    "#score, mse = lstm.evaluate(val_x, val_y, batch_size=1)\n",
    "#print(score + \", \" + mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
