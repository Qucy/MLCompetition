{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9375f50-7821-4aba-ba02-674270499bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 28 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "We will use RAPIDS version 22.12.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np, polars as pl\n",
    "from tqdm.notebook import tqdm\n",
    "import os, sys, pickle, glob, gc\n",
    "from collections import Counter\n",
    "from pandarallel import pandarallel\n",
    "import cudf, itertools\n",
    "\n",
    "pandarallel.initialize(nb_workers=28, progress_bar=False)\n",
    "\n",
    "VER = 1\n",
    "\n",
    "print('We will use RAPIDS version',cudf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1543073-4125-429c-9037-c72872becda9",
   "metadata": {},
   "source": [
    "### Compute Three Co-visitation Matrices with RAPIDSÂ¶\n",
    "We will compute 3 co-visitation matrices using RAPIDS cuDF on GPU. This is 30x faster than using Pandas CPU like other public notebooks! For maximum speed, set the variable DISK_PIECES to the smallest number possible based on the GPU you are using without incurring memory errors. If you run this code offline with 32GB GPU ram, then you can use DISK_PIECES = 1 and compute each co-visitation matrix in almost 1 minute! Kaggle's GPU only has 16GB ram, so we use DISK_PIECES = 4 and it takes an amazing 3 minutes each! Below are some of the tricks to speed up computation\n",
    "\n",
    "Use RAPIDS cuDF GPU instead of Pandas CPU\n",
    "Read disk once and save in CPU RAM for later GPU multiple use\n",
    "Process largest amount of data possible on GPU at one time\n",
    "Merge data in two stages. Multiple small to single medium. Multiple medium to single large.\n",
    "Write result as parquet instead of dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "572bff84-a430-4c55-97ba-d85015e4259b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will process 120 files, in groups of 5 and chunks of 20.\n",
      "CPU times: user 10.1 s, sys: 4.62 s, total: 14.7 s\n",
      "Wall time: 19.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# CACHE FUNCTIONS\n",
    "def read_file(f):\n",
    "    return cudf.DataFrame( data_cache[f] )\n",
    "def read_file_to_cache(f):\n",
    "    df = pd.read_parquet(f)\n",
    "    df.ts = (df.ts/1000).astype('int32')\n",
    "    df['type'] = df['type'].map(type_labels).astype('int8')\n",
    "    return df\n",
    "\n",
    "# CACHE THE DATA ON CPU BEFORE PROCESSING ON GPU\n",
    "data_cache = {}\n",
    "type_labels = {'clicks':0, 'carts':1, 'orders':2}\n",
    "files = glob.glob('./val_data/*_parquet/*')\n",
    "for f in files: data_cache[f] = read_file_to_cache(f)\n",
    "\n",
    "# CHUNK PARAMETERS\n",
    "READ_CT = 5\n",
    "CHUNK = int( np.ceil( len(files)/6 ))\n",
    "print(f'We will process {len(files)} files, in groups of {READ_CT} and chunks of {CHUNK}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79e6600-7423-43ce-9ebe-6ee563caf180",
   "metadata": {},
   "source": [
    "### 1) \"Carts Orders\" Co-visitation Matrix - Type Weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ed358fd-ec4f-412c-86bf-87e77d41b307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### DISK PART 1\n",
      "Processing files 0 thru 19 in groups of 5...\n",
      "0 , 5 , 10 , 15 , \n",
      "Processing files 20 thru 39 in groups of 5...\n",
      "20 , 25 , 30 , 35 , \n",
      "Processing files 40 thru 59 in groups of 5...\n",
      "40 , 45 , 50 , 55 , \n",
      "Processing files 60 thru 79 in groups of 5...\n",
      "60 , 65 , 70 , 75 , \n",
      "Processing files 80 thru 99 in groups of 5...\n",
      "80 , 85 , 90 , 95 , \n",
      "Processing files 100 thru 119 in groups of 5...\n",
      "100 , 105 , 110 , 115 , \n",
      "\n",
      "### DISK PART 2\n",
      "Processing files 0 thru 19 in groups of 5...\n",
      "0 , 5 , 10 , 15 , \n",
      "Processing files 20 thru 39 in groups of 5...\n",
      "20 , 25 , 30 , 35 , \n",
      "Processing files 40 thru 59 in groups of 5...\n",
      "40 , 45 , 50 , 55 , \n",
      "Processing files 60 thru 79 in groups of 5...\n",
      "60 , 65 , 70 , 75 , \n",
      "Processing files 80 thru 99 in groups of 5...\n",
      "80 , 85 , 90 , 95 , \n",
      "Processing files 100 thru 119 in groups of 5...\n",
      "100 , 105 , 110 , 115 , \n",
      "CPU times: user 27.2 s, sys: 18.3 s, total: 45.6 s\n",
      "Wall time: 46.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "type_weight = {0:0.5, 1:9, 2:.5}\n",
    "topn = 40\n",
    "\n",
    "# USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n",
    "DISK_PIECES = 2\n",
    "SIZE = 1.86e6/DISK_PIECES\n",
    "\n",
    "# COMPUTE IN PARTS FOR MEMORY MANGEMENT\n",
    "for PART in range(DISK_PIECES):\n",
    "    print()\n",
    "    print('### DISK PART',PART+1)\n",
    "    \n",
    "    # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n",
    "    # => OUTER CHUNKS\n",
    "    for j in range(6):\n",
    "        a = j*CHUNK\n",
    "        b = min( (j+1)*CHUNK, len(files) )\n",
    "        print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n",
    "        \n",
    "        # => INNER CHUNKS\n",
    "        for k in range(a,b,READ_CT):\n",
    "            # READ FILE\n",
    "            df = [read_file(files[k])]\n",
    "            for i in range(1,READ_CT): \n",
    "                if k+i<b: df.append( read_file(files[k+i]) )\n",
    "            df = cudf.concat(df,ignore_index=True,axis=0)\n",
    "            df = df.sort_values(['session','ts'],ascending=[True,False])\n",
    "            # USE TAIL OF SESSION\n",
    "            df = df.reset_index(drop=True)\n",
    "            df['n'] = df.groupby('session').cumcount()\n",
    "            df = df.loc[df.n<30].drop('n',axis=1)\n",
    "            # CREATE PAIRS\n",
    "            df = df.merge(df,on='session')\n",
    "            df = df.loc[ ((df.ts_x - df.ts_y).abs()< 24 * 60 * 60) & (df.aid_x != df.aid_y) ]\n",
    "            # MEMORY MANAGEMENT COMPUTE IN PARTS\n",
    "            df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n",
    "            # ASSIGN WEIGHTS\n",
    "            df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "            df['wgt'] = df.type_y.map(type_weight)\n",
    "            df = df[['aid_x','aid_y','wgt']]\n",
    "            df.wgt = df.wgt.astype('float32')\n",
    "            df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "            # COMBINE INNER CHUNKS\n",
    "            if k==a: tmp2 = df\n",
    "            else: tmp2 = tmp2.add(df, fill_value=0)\n",
    "            print(k,', ',end='')\n",
    "        print()\n",
    "        # COMBINE OUTER CHUNKS\n",
    "        if a==0: tmp = tmp2\n",
    "        else: tmp = tmp.add(tmp2, fill_value=0)\n",
    "        del tmp2, df\n",
    "        gc.collect()\n",
    "    # CONVERT MATRIX TO DICTIONARY\n",
    "    tmp = tmp.reset_index()\n",
    "    tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "    # SAVE TOP N\n",
    "    tmp = tmp.reset_index(drop=True)\n",
    "    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "    tmp = tmp.loc[tmp.n<topn].drop('n',axis=1)\n",
    "    # SAVE PART TO DISK (convert to pandas first uses less memory)\n",
    "    tmp.to_pandas().to_parquet(f'./val_co_visitation_matrix/top_{topn}_carts_orders_v{VER}_{PART}.pqt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0da3fc6-c1dc-4de5-a7bc-d36720b6fb74",
   "metadata": {},
   "source": [
    "### 2) \"Buy2Buy\" Co-visitation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a758b389-100a-4be9-b9b7-fb71deef15bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### DISK PART 1\n",
      "Processing files 0 thru 19 in groups of 5...\n",
      "0 , 5 , 10 , 15 , \n",
      "Processing files 20 thru 39 in groups of 5...\n",
      "20 , 25 , 30 , 35 , \n",
      "Processing files 40 thru 59 in groups of 5...\n",
      "40 , 45 , 50 , 55 , \n",
      "Processing files 60 thru 79 in groups of 5...\n",
      "60 , 65 , 70 , 75 , \n",
      "Processing files 80 thru 99 in groups of 5...\n",
      "80 , 85 , 90 , 95 , \n",
      "Processing files 100 thru 119 in groups of 5...\n",
      "100 , 105 , 110 , 115 , \n",
      "CPU times: user 4.84 s, sys: 4.06 s, total: 8.91 s\n",
      "Wall time: 9.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n",
    "DISK_PIECES = 1\n",
    "SIZE = 1.86e6/DISK_PIECES\n",
    "\n",
    "# COMPUTE IN PARTS FOR MEMORY MANGEMENT\n",
    "for PART in range(DISK_PIECES):\n",
    "    print()\n",
    "    print('### DISK PART',PART+1)\n",
    "    \n",
    "    # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n",
    "    # => OUTER CHUNKS\n",
    "    for j in range(6):\n",
    "        a = j*CHUNK\n",
    "        b = min( (j+1)*CHUNK, len(files) )\n",
    "        print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n",
    "        \n",
    "        # => INNER CHUNKS\n",
    "        for k in range(a,b,READ_CT):\n",
    "            # READ FILE\n",
    "            df = [read_file(files[k])]\n",
    "            for i in range(1,READ_CT): \n",
    "                if k+i<b: df.append( read_file(files[k+i]) )\n",
    "            df = cudf.concat(df,ignore_index=True,axis=0)\n",
    "            df = df.loc[df['type'].isin([1,2])] # ONLY WANT CARTS AND ORDERS\n",
    "            df = df.sort_values(['session','ts'],ascending=[True,False])\n",
    "            # USE TAIL OF SESSION\n",
    "            df = df.reset_index(drop=True)\n",
    "            df['n'] = df.groupby('session').cumcount()\n",
    "            df = df.loc[df.n<30].drop('n',axis=1)\n",
    "            # CREATE PAIRS\n",
    "            df = df.merge(df,on='session')\n",
    "            df = df.loc[ ((df.ts_x - df.ts_y).abs()< 14 * 24 * 60 * 60) & (df.aid_x != df.aid_y) ] # 14 DAYS\n",
    "            # MEMORY MANAGEMENT COMPUTE IN PARTS\n",
    "            df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n",
    "            # ASSIGN WEIGHTS\n",
    "            df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "            df['wgt'] = 1\n",
    "            df = df[['aid_x','aid_y','wgt']]\n",
    "            df.wgt = df.wgt.astype('float32')\n",
    "            df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "            # COMBINE INNER CHUNKS\n",
    "            if k==a: tmp2 = df\n",
    "            else: tmp2 = tmp2.add(df, fill_value=0)\n",
    "            print(k,', ',end='')\n",
    "        print()\n",
    "        # COMBINE OUTER CHUNKS\n",
    "        if a==0: tmp = tmp2\n",
    "        else: tmp = tmp.add(tmp2, fill_value=0)\n",
    "        del tmp2, df\n",
    "        gc.collect()\n",
    "    # CONVERT MATRIX TO DICTIONARY\n",
    "    tmp = tmp.reset_index()\n",
    "    tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "    # SAVE TOP 40\n",
    "    tmp = tmp.reset_index(drop=True)\n",
    "    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "    tmp = tmp.loc[tmp.n<topn].drop('n',axis=1)\n",
    "    # SAVE PART TO DISK (convert to pandas first uses less memory)\n",
    "    tmp.to_pandas().to_parquet(f'./val_co_visitation_matrix/top_{topn}_buy2buy_v{VER}_{PART}.pqt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d641fc-473e-4215-89df-3c86780c41a9",
   "metadata": {},
   "source": [
    "### 3) \"Clicks\" Co-visitation Matrix - Time Weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ad5be5c-60c3-4861-b5c3-845f270648ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### DISK PART 1\n",
      "Processing files 0 thru 19 in groups of 5...\n",
      "0 , 5 , 10 , 15 , \n",
      "Processing files 20 thru 39 in groups of 5...\n",
      "20 , 25 , 30 , 35 , \n",
      "Processing files 40 thru 59 in groups of 5...\n",
      "40 , 45 , 50 , 55 , \n",
      "Processing files 60 thru 79 in groups of 5...\n",
      "60 , 65 , 70 , 75 , \n",
      "Processing files 80 thru 99 in groups of 5...\n",
      "80 , 85 , 90 , 95 , \n",
      "Processing files 100 thru 119 in groups of 5...\n",
      "100 , 105 , 110 , 115 , \n",
      "\n",
      "### DISK PART 2\n",
      "Processing files 0 thru 19 in groups of 5...\n",
      "0 , 5 , 10 , 15 , \n",
      "Processing files 20 thru 39 in groups of 5...\n",
      "20 , 25 , 30 , 35 , \n",
      "Processing files 40 thru 59 in groups of 5...\n",
      "40 , 45 , 50 , 55 , \n",
      "Processing files 60 thru 79 in groups of 5...\n",
      "60 , 65 , 70 , 75 , \n",
      "Processing files 80 thru 99 in groups of 5...\n",
      "80 , 85 , 90 , 95 , \n",
      "Processing files 100 thru 119 in groups of 5...\n",
      "100 , 105 , 110 , 115 , \n",
      "CPU times: user 26.7 s, sys: 17.6 s, total: 44.2 s\n",
      "Wall time: 45.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n",
    "DISK_PIECES = 2\n",
    "SIZE = 1.86e6/DISK_PIECES\n",
    "\n",
    "# COMPUTE IN PARTS FOR MEMORY MANGEMENT\n",
    "for PART in range(DISK_PIECES):\n",
    "    print()\n",
    "    print('### DISK PART',PART+1)\n",
    "    \n",
    "    # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n",
    "    # => OUTER CHUNKS\n",
    "    for j in range(6):\n",
    "        a = j*CHUNK\n",
    "        b = min( (j+1)*CHUNK, len(files) )\n",
    "        print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n",
    "        \n",
    "        # => INNER CHUNKS\n",
    "        for k in range(a,b,READ_CT):\n",
    "            # READ FILE\n",
    "            df = [read_file(files[k])]\n",
    "            for i in range(1,READ_CT): \n",
    "                if k+i<b: df.append( read_file(files[k+i]) )\n",
    "            df = cudf.concat(df,ignore_index=True,axis=0)\n",
    "            df = df.sort_values(['session','ts'],ascending=[True,False])\n",
    "            # USE TAIL OF SESSION\n",
    "            df = df.reset_index(drop=True)\n",
    "            df['n'] = df.groupby('session').cumcount()\n",
    "            df = df.loc[df.n<30].drop('n',axis=1)\n",
    "            # CREATE PAIRS\n",
    "            df = df.merge(df,on='session')\n",
    "            df = df.loc[ ((df.ts_x - df.ts_y).abs()< 24 * 60 * 60) & (df.aid_x != df.aid_y) ]\n",
    "            # MEMORY MANAGEMENT COMPUTE IN PARTS\n",
    "            df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n",
    "            # ASSIGN WEIGHTS\n",
    "            df = df[['session', 'aid_x', 'aid_y','ts_x']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "            df['wgt'] = 1 + 3*(df.ts_x - 1659304800)/(1662328791-1659304800)\n",
    "            df = df[['aid_x','aid_y','wgt']]\n",
    "            df.wgt = df.wgt.astype('float32')\n",
    "            df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "            # COMBINE INNER CHUNKS\n",
    "            if k==a: tmp2 = df\n",
    "            else: tmp2 = tmp2.add(df, fill_value=0)\n",
    "            print(k,', ',end='')\n",
    "        print()\n",
    "        # COMBINE OUTER CHUNKS\n",
    "        if a==0: tmp = tmp2\n",
    "        else: tmp = tmp.add(tmp2, fill_value=0)\n",
    "        del tmp2, df\n",
    "        gc.collect()\n",
    "    # CONVERT MATRIX TO DICTIONARY\n",
    "    tmp = tmp.reset_index()\n",
    "    tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "    # SAVE TOP 40\n",
    "    tmp = tmp.reset_index(drop=True)\n",
    "    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "    tmp = tmp.loc[tmp.n<topn].drop('n',axis=1)\n",
    "    # SAVE PART TO DISK (convert to pandas first uses less memory)\n",
    "    tmp.to_pandas().to_parquet(f'./val_co_visitation_matrix/top_{topn}_clicks_v{VER}_{PART}.pqt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e03d1877-f233-4b53-882f-1cfdc3bacda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FREE MEMORY\n",
    "del data_cache, tmp\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b710209b-4735-4715-8048-3675f7ccab54",
   "metadata": {},
   "source": [
    "### Step 2 - ReRank (choose 20) using handcrafted rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a649023-f170-4e57-b80e-ceb011123f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data has shape (7683577, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>aid</th>\n",
       "      <th>ts</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11098528</td>\n",
       "      <td>11830</td>\n",
       "      <td>1661119200</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11098529</td>\n",
       "      <td>1105029</td>\n",
       "      <td>1661119200</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11098530</td>\n",
       "      <td>264500</td>\n",
       "      <td>1661119200</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11098530</td>\n",
       "      <td>264500</td>\n",
       "      <td>1661119288</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11098530</td>\n",
       "      <td>409236</td>\n",
       "      <td>1661119369</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    session      aid          ts  type\n",
       "0  11098528    11830  1661119200     0\n",
       "1  11098529  1105029  1661119200     0\n",
       "2  11098530   264500  1661119200     0\n",
       "3  11098530   264500  1661119288     0\n",
       "4  11098530   409236  1661119369     0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_test():    \n",
    "    dfs = []\n",
    "    for e, chunk_file in enumerate(glob.glob('./val_data/test_parquet/*')):\n",
    "        chunk = pd.read_parquet(chunk_file)\n",
    "        chunk.ts = (chunk.ts/1000).astype('int32')\n",
    "        chunk['type'] = chunk['type'].map(type_labels).astype('int8')\n",
    "        dfs.append(chunk)\n",
    "    return pd.concat(dfs).reset_index(drop=True)\n",
    "\n",
    "test_df = load_test()\n",
    "print('Test data has shape',test_df.shape)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fca8f0e6-48de-4d85-b86d-7f5d9c1b8df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are size of our 3 co-visitation matrices:\n",
      "1812132 1812132 1055146\n",
      "CPU times: user 16.3 s, sys: 1.87 s, total: 18.1 s\n",
      "Wall time: 11.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# LOAD THREE CO-VISITATION MATRICES\n",
    "def pqt_to_dict(path):\n",
    "    return pl.read_parquet(path).groupby('aid_x').agg(pl.col('aid_y').list()).to_pandas().set_index('aid_x').aid_y.apply(list).to_dict()\n",
    "\n",
    "# load topn clicks co visitation matrix\n",
    "topn_clicks = pqt_to_dict(f'./val_co_visitation_matrix/top_{topn}_clicks_v{VER}_0.pqt')\n",
    "for k in range(1, DISK_PIECES): \n",
    "    topn_clicks.update( pqt_to_dict(f'./val_co_visitation_matrix/top_{topn}_clicks_v{VER}_{k}.pqt') )\n",
    "\n",
    "# load topn buys co visitation matrix\n",
    "topn_buys = pqt_to_dict(f'./val_co_visitation_matrix/top_{topn}_carts_orders_v{VER}_0.pqt')\n",
    "for k in range(1, DISK_PIECES): \n",
    "    topn_buys.update( pqt_to_dict(f'./val_co_visitation_matrix/top_{topn}_carts_orders_v{VER}_{k}.pqt') )\n",
    "    \n",
    "# load topn b2b co visitation matrix\n",
    "topn_b2b = pqt_to_dict(f'./val_co_visitation_matrix/top_{topn}_buy2buy_v{VER}_0.pqt')\n",
    "\n",
    "# TOP CLICKS AND ORDERS IN TEST\n",
    "top_clicks = test_df.loc[test_df['type']=='clicks','aid'].value_counts().index.values[:40]\n",
    "top_orders = test_df.loc[test_df['type']=='orders','aid'].value_counts().index.values[:40]\n",
    "\n",
    "print('Here are size of our 3 co-visitation matrices:')\n",
    "print( len( topn_clicks ), len( topn_buys ), len( topn_b2b ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5d47f4c-c7b6-425d-84e0-350e5a826899",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_weight_multipliers = {0: 1, 1: 6, 2: 3}\n",
    "\n",
    "topn = 100\n",
    "\n",
    "def suggest_clicks(df):\n",
    "    # USE USER HISTORY AIDS AND TYPES\n",
    "    aids=df.aid.tolist()\n",
    "    types = df.type.tolist()\n",
    "    unique_aids = list(dict.fromkeys(aids[::-1] ))\n",
    "    # RERANK CANDIDATES USING WEIGHTS\n",
    "    if len(unique_aids)>=topn:\n",
    "        weights=np.logspace(0.1,1,len(aids),base=2, endpoint=True)-1\n",
    "        aids_temp = Counter() \n",
    "        # RERANK BASED ON REPEAT ITEMS AND TYPE OF ITEMS\n",
    "        for aid,w,t in zip(aids,weights,types): \n",
    "            aids_temp[aid] += w * type_weight_multipliers[t]\n",
    "        sorted_aids = [k for k,v in aids_temp.most_common(topn)]\n",
    "        return sorted_aids\n",
    "    # USE \"CLICKS\" CO-VISITATION MATRIX\n",
    "    aids2 = list(itertools.chain(*[topn_clicks[aid] for aid in unique_aids if aid in topn_clicks]))\n",
    "    # RERANK CANDIDATES\n",
    "    top_aids2 = [aid2 for aid2, cnt in Counter(aids2).most_common(topn) if aid2 not in unique_aids]\n",
    "    result = unique_aids + top_aids2[:topn - len(unique_aids)]\n",
    "    # USE TOP20 TEST CLICKS\n",
    "    return result + list(top_clicks)[:topn-len(result)]\n",
    "\n",
    "def suggest_buys(df):\n",
    "    # USE USER HISTORY AIDS AND TYPES\n",
    "    aids=df.aid.tolist()\n",
    "    types = df.type.tolist()\n",
    "    # UNIQUE AIDS AND UNIQUE BUYS\n",
    "    unique_aids = list(dict.fromkeys(aids[::-1] ))\n",
    "    df = df.loc[(df['type']==1)|(df['type']==2)]\n",
    "    unique_buys = list(dict.fromkeys( df.aid.tolist()[::-1] ))\n",
    "    # RERANK CANDIDATES USING WEIGHTS\n",
    "    if len(unique_aids)>=topn:\n",
    "        weights=np.logspace(0.5,1,len(aids),base=2, endpoint=True)-1\n",
    "        aids_temp = Counter() \n",
    "        # RERANK BASED ON REPEAT ITEMS AND TYPE OF ITEMS\n",
    "        for aid,w,t in zip(aids,weights,types): \n",
    "            aids_temp[aid] += w * type_weight_multipliers[t]\n",
    "        # RERANK CANDIDATES USING \"BUY2BUY\" CO-VISITATION MATRIX\n",
    "        aids3 = list(itertools.chain(*[topn_b2b[aid] for aid in unique_buys if aid in topn_b2b]))\n",
    "        for aid in aids3: aids_temp[aid] += 0.1\n",
    "        sorted_aids = [k for k,v in aids_temp.most_common(topn)]\n",
    "        return sorted_aids\n",
    "    # USE \"CART ORDER\" CO-VISITATION MATRIX\n",
    "    aids2 = list(itertools.chain(*[topn_buys[aid] for aid in unique_aids if aid in topn_buys]))\n",
    "    # USE \"BUY2BUY\" CO-VISITATION MATRIX\n",
    "    aids3 = list(itertools.chain(*[topn_b2b[aid] for aid in unique_buys if aid in topn_b2b]))\n",
    "    # RERANK CANDIDATES\n",
    "    top_aids2 = [aid2 for aid2, cnt in Counter(aids2+aids3).most_common(topn) if aid2 not in unique_aids] \n",
    "    result = unique_aids + top_aids2[:topn - len(unique_aids)]\n",
    "    # USE TOP20 TEST ORDERS\n",
    "    return result + list(top_orders)[:topn-len(result)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6f9c8c-2c0c-4e20-a5e5-380084da42a3",
   "metadata": {},
   "source": [
    "### Create submission csv to validate cv score\n",
    "- Inferring test data with Pandas groupby is slow. We use pandarallel to accelerate process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e54158c9-9096-4326-b22d-707dc703fd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/kaggle-env/lib/python3.9/site-packages/pandarallel/data_types/dataframe_groupby.py:18: FutureWarning: In a future version of pandas, a length 1 tuple will be returned when iterating over a groupby with a grouper equal to a list of length 1. Don't supply a list with a single grouper to avoid this warning.\n",
      "  iterator = iter(dataframe_groupby)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49 s, sys: 5.11 s, total: 54.2 s\n",
      "Wall time: 1min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pred_df_clicks = test_df.sort_values([\"session\", \"ts\"]).groupby([\"session\"]).parallel_apply(\n",
    "    lambda x: suggest_clicks(x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "693cf960-5e35-4926-8ac1-eb2ace1556b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/kaggle-env/lib/python3.9/site-packages/pandarallel/data_types/dataframe_groupby.py:18: FutureWarning: In a future version of pandas, a length 1 tuple will be returned when iterating over a groupby with a grouper equal to a list of length 1. Don't supply a list with a single grouper to avoid this warning.\n",
      "  iterator = iter(dataframe_groupby)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 59.4 s, sys: 7.83 s, total: 1min 7s\n",
      "Wall time: 2min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pred_df_buys = test_df.sort_values([\"session\", \"ts\"]).groupby([\"session\"]).parallel_apply(\n",
    "    lambda x: suggest_buys(x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a543a36-b513-4068-9399-1c312a85c59f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_type</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11098528_clicks</td>\n",
       "      <td>11830 588923 1732105 571762 884502 1157882 876...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11098529_clicks</td>\n",
       "      <td>1105029 459126 1339838 1544564 217742 1694360 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11098530_clicks</td>\n",
       "      <td>409236 264500 1603001 963957 254154 583026 167...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11098531_clicks</td>\n",
       "      <td>396199 1271998 452188 1728212 1365569 624163 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11098532_clicks</td>\n",
       "      <td>876469 7651 108125 612920 1202618 1159379 7790...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      session_type                                             labels\n",
       "0  11098528_clicks  11830 588923 1732105 571762 884502 1157882 876...\n",
       "1  11098529_clicks  1105029 459126 1339838 1544564 217742 1694360 ...\n",
       "2  11098530_clicks  409236 264500 1603001 963957 254154 583026 167...\n",
       "3  11098531_clicks  396199 1271998 452188 1728212 1365569 624163 1...\n",
       "4  11098532_clicks  876469 7651 108125 612920 1202618 1159379 7790..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate submission dataframe\n",
    "clicks_pred_df = pd.DataFrame(pred_df_clicks.add_suffix(\"_clicks\"), columns=[\"labels\"]).reset_index()\n",
    "orders_pred_df = pd.DataFrame(pred_df_buys.add_suffix(\"_orders\"), columns=[\"labels\"]).reset_index()\n",
    "carts_pred_df = pd.DataFrame(pred_df_buys.add_suffix(\"_carts\"), columns=[\"labels\"]).reset_index()\n",
    "\n",
    "pred_df = pd.concat([clicks_pred_df, orders_pred_df, carts_pred_df])\n",
    "pred_df.columns = [\"session_type\", \"labels\"]\n",
    "pred_df[\"labels\"] = pred_df.labels.apply(lambda x: \" \".join(map(str,x)))\n",
    "# pred_df.to_csv(\"validation_preds.csv\", index=False)\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bbf881-98b2-452e-b9d6-cd802021c2f0",
   "metadata": {},
   "source": [
    "### Compute Validation Score\n",
    "This code is from Radek here. It has been modified to use less memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d44a129-61da-41f3-86eb-28080bd368a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clicks recall = 0.620077423735456\n",
      "carts recall = 0.49115323635430075\n",
      "orders recall = 0.6999135022645809\n",
      "=============\n",
      "Overall Recall = 0.6293018146385844\n",
      "=============\n",
      "CPU times: user 49.6 s, sys: 2.16 s, total: 51.7 s\n",
      "Wall time: 51.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# COMPUTE METRIC\n",
    "score = 0\n",
    "weights = {'clicks': 0.10, 'carts': 0.30, 'orders': 0.60}\n",
    "for t in ['clicks','carts','orders']:\n",
    "    sub = pred_df.loc[pred_df.session_type.str.contains(t)].copy()\n",
    "    sub['session'] = sub.session_type.apply(lambda x: int(x.split('_')[0]))\n",
    "    sub.labels = sub.labels.apply(lambda x: [int(i) for i in x.split(' ')[:topn]])\n",
    "    test_labels = pd.read_parquet('./val_data/test_labels.parquet')\n",
    "    test_labels = test_labels.loc[test_labels['type']==t]\n",
    "    test_labels = test_labels.merge(sub, how='left', on=['session'])\n",
    "    test_labels['hits'] = test_labels.apply(lambda df: len(set(df.ground_truth).intersection(set(df.labels))), axis=1)\n",
    "    test_labels['gt_count'] = test_labels.ground_truth.str.len().clip(0,20)\n",
    "    recall = test_labels['hits'].sum() / test_labels['gt_count'].sum()\n",
    "    score += weights[t]*recall\n",
    "    print(f'{t} recall =',recall)\n",
    "    \n",
    "print('=============')\n",
    "print('Overall Recall =',score)\n",
    "print('=============')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "795b0d14-4ed5-44df-aa1e-2f1cbeee50d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.54 s, sys: 1.74 s, total: 8.28 s\n",
      "Wall time: 11.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# write candidates to local disk for stage 2 model\n",
    "pred_df.to_parquet('./val_data/cv_candidates.pgt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7001a78e-cf97-43b8-9fd8-b8350d1a01b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
