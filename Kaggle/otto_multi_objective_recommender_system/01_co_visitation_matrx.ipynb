{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9375f50-7821-4aba-ba02-674270499bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 28 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "We will use RAPIDS version 22.12.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np, polars as pl\n",
    "from tqdm.notebook import tqdm\n",
    "import os, sys, pickle, glob, gc\n",
    "from collections import Counter\n",
    "from pandarallel import pandarallel\n",
    "import cudf, itertools\n",
    "\n",
    "pandarallel.initialize(nb_workers=28, progress_bar=False)\n",
    "\n",
    "# memory management by default to use 32 int or float only\n",
    "cudf.set_option(\"default_integer_bitwidth\", 32)\n",
    "cudf.set_option(\"default_float_bitwidth\", 32)\n",
    "\n",
    "# init variable\n",
    "VER = 1\n",
    "topn = 40\n",
    "is_validation = False # if generate for validation dataset set to True otherwise set to False to generate on full dataset\n",
    "data_cache = {}\n",
    "type_labels = {'clicks':0, 'carts':1, 'orders':2}\n",
    "\n",
    "if is_validation:\n",
    "    # parquet file path\n",
    "    data_path = './val_data/*_parquet/*parquet'\n",
    "    test_data_path = './val_data/test_parquet/*parquet'\n",
    "    output_path = 'val_co_visitation_matrix'\n",
    "else:\n",
    "    data_path = './data/*_parquet/*parquet'\n",
    "    test_data_path = './data/test_parquet/*parquet'\n",
    "    output_path = 'co_visitation_matrix'\n",
    "\n",
    "print('We will use RAPIDS version',cudf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1543073-4125-429c-9037-c72872becda9",
   "metadata": {},
   "source": [
    "### Compute Three Co-visitation Matrices with RAPIDSÂ¶\n",
    "We will compute 3 co-visitation matrices using RAPIDS cuDF on GPU. This is 30x faster than using Pandas CPU like other public notebooks! For maximum speed, set the variable DISK_PIECES to the smallest number possible based on the GPU you are using without incurring memory errors. If you run this code offline with 32GB GPU ram, then you can use DISK_PIECES = 1 and compute each co-visitation matrix in almost 1 minute! Kaggle's GPU only has 16GB ram, so we use DISK_PIECES = 4 and it takes an amazing 3 minutes each! Below are some of the tricks to speed up computation\n",
    "\n",
    "Use RAPIDS cuDF GPU instead of Pandas CPU\n",
    "Read disk once and save in CPU RAM for later GPU multiple use\n",
    "Process largest amount of data possible on GPU at one time\n",
    "Merge data in two stages. Multiple small to single medium. Multiple medium to single large.\n",
    "Write result as parquet instead of dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "572bff84-a430-4c55-97ba-d85015e4259b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will process 146 files, in groups of 5 and chunks of 25.\n",
      "CPU times: user 11.5 s, sys: 4.59 s, total: 16.1 s\n",
      "Wall time: 9.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# CACHE FUNCTIONS\n",
    "def read_file(f):\n",
    "    return cudf.DataFrame( data_cache[f] )\n",
    "def read_file_to_cache(f):\n",
    "    df = pd.read_parquet(f)\n",
    "    df.ts = (df.ts/1000).astype('int32')\n",
    "    df['type'] = df['type'].map(type_labels).astype('int8')\n",
    "    return df\n",
    "\n",
    "# CACHE THE DATA ON CPU BEFORE PROCESSING ON GPU\n",
    "files = glob.glob(data_path)\n",
    "for f in files: data_cache[f] = read_file_to_cache(f)\n",
    "\n",
    "# CHUNK PARAMETERS\n",
    "READ_CT = 5\n",
    "CHUNK = int( np.ceil( len(files)/6 ))\n",
    "print(f'We will process {len(files)} files, in groups of {READ_CT} and chunks of {CHUNK}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79e6600-7423-43ce-9ebe-6ee563caf180",
   "metadata": {},
   "source": [
    "### 1) \"Carts Orders\" Co-visitation Matrix - Type Weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ed358fd-ec4f-412c-86bf-87e77d41b307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### DISK PART 1\n",
      "Processing files 0 thru 24 in groups of 5...\n",
      "0 , 5 , 10 , 15 , 20 , \n",
      "Processing files 25 thru 49 in groups of 5...\n",
      "25 , 30 , 35 , 40 , 45 , \n",
      "Processing files 50 thru 74 in groups of 5...\n",
      "50 , 55 , 60 , 65 , 70 , \n",
      "Processing files 75 thru 99 in groups of 5...\n",
      "75 , 80 , 85 , 90 , 95 , \n",
      "Processing files 100 thru 124 in groups of 5...\n",
      "100 , 105 , 110 , 115 , 120 , \n",
      "Processing files 125 thru 145 in groups of 5...\n",
      "125 , 130 , 135 , 140 , 145 , \n",
      "\n",
      "### DISK PART 2\n",
      "Processing files 0 thru 24 in groups of 5...\n",
      "0 , 5 , 10 , 15 , 20 , \n",
      "Processing files 25 thru 49 in groups of 5...\n",
      "25 , 30 , 35 , 40 , 45 , \n",
      "Processing files 50 thru 74 in groups of 5...\n",
      "50 , 55 , 60 , 65 , 70 , \n",
      "Processing files 75 thru 99 in groups of 5...\n",
      "75 , 80 , 85 , 90 , 95 , \n",
      "Processing files 100 thru 124 in groups of 5...\n",
      "100 , 105 , 110 , 115 , 120 , \n",
      "Processing files 125 thru 145 in groups of 5...\n",
      "125 , 130 , 135 , 140 , 145 , \n",
      "\n",
      "### DISK PART 3\n",
      "Processing files 0 thru 24 in groups of 5...\n",
      "0 , 5 , 10 , 15 , 20 , \n",
      "Processing files 25 thru 49 in groups of 5...\n",
      "25 , 30 , 35 , 40 , 45 , \n",
      "Processing files 50 thru 74 in groups of 5...\n",
      "50 , 55 , 60 , 65 , 70 , \n",
      "Processing files 75 thru 99 in groups of 5...\n",
      "75 , 80 , 85 , 90 , 95 , \n",
      "Processing files 100 thru 124 in groups of 5...\n",
      "100 , 105 , 110 , 115 , 120 , \n",
      "Processing files 125 thru 145 in groups of 5...\n",
      "125 , 130 , 135 , 140 , 145 , \n",
      "CPU times: user 50.6 s, sys: 32.7 s, total: 1min 23s\n",
      "Wall time: 1min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "type_weight = {0:0.5, 1:9, 2:.5}\n",
    "\n",
    "# USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n",
    "DISK_PIECES = 3\n",
    "SIZE = 1.86e6/DISK_PIECES\n",
    "\n",
    "# COMPUTE IN PARTS FOR MEMORY MANGEMENT\n",
    "for PART in range(DISK_PIECES):\n",
    "    print()\n",
    "    print('### DISK PART',PART+1)\n",
    "    \n",
    "    # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n",
    "    # => OUTER CHUNKS\n",
    "    for j in range(6):\n",
    "        a = j*CHUNK\n",
    "        b = min( (j+1)*CHUNK, len(files) )\n",
    "        print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n",
    "        \n",
    "        # => INNER CHUNKS\n",
    "        for k in range(a,b,READ_CT):\n",
    "            # READ FILE\n",
    "            df = [read_file(files[k])]\n",
    "            for i in range(1,READ_CT): \n",
    "                if k+i<b: df.append( read_file(files[k+i]) )\n",
    "            df = cudf.concat(df,ignore_index=True,axis=0)\n",
    "            df = df.sort_values(['session','ts'],ascending=[True,False])\n",
    "            # USE TAIL OF SESSION\n",
    "            df = df.reset_index(drop=True)\n",
    "            df['n'] = df.groupby('session').cumcount()\n",
    "            df = df.loc[df.n<30].drop('n',axis=1)\n",
    "            # CREATE PAIRS\n",
    "            df = df.merge(df,on='session')\n",
    "            df = df.loc[ ((df.ts_x - df.ts_y).abs()< 24 * 60 * 60) & (df.aid_x != df.aid_y) ]\n",
    "            # MEMORY MANAGEMENT COMPUTE IN PARTS\n",
    "            df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n",
    "            # ASSIGN WEIGHTS\n",
    "            df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "            df['wgt'] = df.type_y.map(type_weight)\n",
    "            df = df[['aid_x','aid_y','wgt']]\n",
    "            df.wgt = df.wgt.astype('float32')\n",
    "            df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "            # COMBINE INNER CHUNKS\n",
    "            if k==a: tmp2 = df\n",
    "            else: tmp2 = tmp2.add(df, fill_value=0)\n",
    "            print(k,', ',end='')\n",
    "        print()\n",
    "        # COMBINE OUTER CHUNKS\n",
    "        if a==0: tmp = tmp2\n",
    "        else: tmp = tmp.add(tmp2, fill_value=0)\n",
    "        del tmp2, df\n",
    "        gc.collect()\n",
    "    # CONVERT MATRIX TO DICTIONARY\n",
    "    tmp = tmp.reset_index()\n",
    "    tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "    # SAVE TOP N\n",
    "    tmp = tmp.reset_index(drop=True)\n",
    "    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "    tmp = tmp.loc[tmp.n<topn].drop('n',axis=1)\n",
    "    # SAVE PART TO DISK (convert to pandas first uses less memory)\n",
    "    tmp.to_pandas().to_parquet(f'./{output_path}/top_{topn}_carts_orders_v{VER}_{PART}.pqt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0da3fc6-c1dc-4de5-a7bc-d36720b6fb74",
   "metadata": {},
   "source": [
    "### 2) \"Buy2Buy\" Co-visitation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a758b389-100a-4be9-b9b7-fb71deef15bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### DISK PART 1\n",
      "Processing files 0 thru 24 in groups of 5...\n",
      "0 , 5 , 10 , 15 , 20 , \n",
      "Processing files 25 thru 49 in groups of 5...\n",
      "25 , 30 , 35 , 40 , 45 , \n",
      "Processing files 50 thru 74 in groups of 5...\n",
      "50 , 55 , 60 , 65 , 70 , \n",
      "Processing files 75 thru 99 in groups of 5...\n",
      "75 , 80 , 85 , 90 , 95 , \n",
      "Processing files 100 thru 124 in groups of 5...\n",
      "100 , 105 , 110 , 115 , 120 , \n",
      "Processing files 125 thru 145 in groups of 5...\n",
      "125 , 130 , 135 , 140 , 145 , \n",
      "CPU times: user 9.13 s, sys: 5.52 s, total: 14.7 s\n",
      "Wall time: 15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n",
    "DISK_PIECES = 1\n",
    "SIZE = 1.86e6/DISK_PIECES\n",
    "\n",
    "# COMPUTE IN PARTS FOR MEMORY MANGEMENT\n",
    "for PART in range(DISK_PIECES):\n",
    "    print()\n",
    "    print('### DISK PART',PART+1)\n",
    "    \n",
    "    # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n",
    "    # => OUTER CHUNKS\n",
    "    for j in range(6):\n",
    "        a = j*CHUNK\n",
    "        b = min( (j+1)*CHUNK, len(files) )\n",
    "        print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n",
    "        \n",
    "        # => INNER CHUNKS\n",
    "        for k in range(a,b,READ_CT):\n",
    "            # READ FILE\n",
    "            df = [read_file(files[k])]\n",
    "            for i in range(1,READ_CT): \n",
    "                if k+i<b: df.append( read_file(files[k+i]) )\n",
    "            df = cudf.concat(df,ignore_index=True,axis=0)\n",
    "            df = df.loc[df['type'].isin([1,2])] # ONLY WANT CARTS AND ORDERS\n",
    "            df = df.sort_values(['session','ts'],ascending=[True,False])\n",
    "            # USE TAIL OF SESSION\n",
    "            df = df.reset_index(drop=True)\n",
    "            df['n'] = df.groupby('session').cumcount()\n",
    "            df = df.loc[df.n<30].drop('n',axis=1)\n",
    "            # CREATE PAIRS\n",
    "            df = df.merge(df,on='session')\n",
    "            df = df.loc[ ((df.ts_x - df.ts_y).abs()< 14 * 24 * 60 * 60) & (df.aid_x != df.aid_y) ] # 14 DAYS\n",
    "            # MEMORY MANAGEMENT COMPUTE IN PARTS\n",
    "            df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n",
    "            # ASSIGN WEIGHTS\n",
    "            df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "            df['wgt'] = 1\n",
    "            df = df[['aid_x','aid_y','wgt']]\n",
    "            df.wgt = df.wgt.astype('float32')\n",
    "            df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "            # COMBINE INNER CHUNKS\n",
    "            if k==a: tmp2 = df\n",
    "            else: tmp2 = tmp2.add(df, fill_value=0)\n",
    "            print(k,', ',end='')\n",
    "        print()\n",
    "        # COMBINE OUTER CHUNKS\n",
    "        if a==0: tmp = tmp2\n",
    "        else: tmp = tmp.add(tmp2, fill_value=0)\n",
    "        del tmp2, df\n",
    "        gc.collect()\n",
    "    # CONVERT MATRIX TO DICTIONARY\n",
    "    tmp = tmp.reset_index()\n",
    "    tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "    # SAVE TOP 40\n",
    "    tmp = tmp.reset_index(drop=True)\n",
    "    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "    tmp = tmp.loc[tmp.n<topn].drop('n',axis=1)\n",
    "    # SAVE PART TO DISK (convert to pandas first uses less memory)\n",
    "    tmp.to_pandas().to_parquet(f'./{output_path}/top_{topn}_buy2buy_v{VER}_{PART}.pqt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d641fc-473e-4215-89df-3c86780c41a9",
   "metadata": {},
   "source": [
    "### 3) \"Clicks\" Co-visitation Matrix - Time Weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ad5be5c-60c3-4861-b5c3-845f270648ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### DISK PART 1\n",
      "Processing files 0 thru 24 in groups of 5...\n",
      "0 , 5 , 10 , 15 , 20 , \n",
      "Processing files 25 thru 49 in groups of 5...\n",
      "25 , 30 , 35 , 40 , 45 , \n",
      "Processing files 50 thru 74 in groups of 5...\n",
      "50 , 55 , 60 , 65 , 70 , \n",
      "Processing files 75 thru 99 in groups of 5...\n",
      "75 , 80 , 85 , 90 , 95 , \n",
      "Processing files 100 thru 124 in groups of 5...\n",
      "100 , 105 , 110 , 115 , 120 , \n",
      "Processing files 125 thru 145 in groups of 5...\n",
      "125 , 130 , 135 , 140 , 145 , \n",
      "\n",
      "### DISK PART 2\n",
      "Processing files 0 thru 24 in groups of 5...\n",
      "0 , 5 , 10 , 15 , 20 , \n",
      "Processing files 25 thru 49 in groups of 5...\n",
      "25 , 30 , 35 , 40 , 45 , \n",
      "Processing files 50 thru 74 in groups of 5...\n",
      "50 , 55 , 60 , 65 , 70 , \n",
      "Processing files 75 thru 99 in groups of 5...\n",
      "75 , 80 , 85 , 90 , 95 , \n",
      "Processing files 100 thru 124 in groups of 5...\n",
      "100 , 105 , 110 , 115 , 120 , \n",
      "Processing files 125 thru 145 in groups of 5...\n",
      "125 , 130 , 135 , 140 , 145 , \n",
      "\n",
      "### DISK PART 3\n",
      "Processing files 0 thru 24 in groups of 5...\n",
      "0 , 5 , 10 , 15 , 20 , \n",
      "Processing files 25 thru 49 in groups of 5...\n",
      "25 , 30 , 35 , 40 , 45 , \n",
      "Processing files 50 thru 74 in groups of 5...\n",
      "50 , 55 , 60 , 65 , 70 , \n",
      "Processing files 75 thru 99 in groups of 5...\n",
      "75 , 80 , 85 , 90 , 95 , \n",
      "Processing files 100 thru 124 in groups of 5...\n",
      "100 , 105 , 110 , 115 , 120 , \n",
      "Processing files 125 thru 145 in groups of 5...\n",
      "125 , 130 , 135 , 140 , 145 , \n",
      "CPU times: user 50.5 s, sys: 30.9 s, total: 1min 21s\n",
      "Wall time: 1min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# USE SMALLEST DISK_PIECES POSSIBLE WITHOUT MEMORY ERROR\n",
    "DISK_PIECES = 3\n",
    "SIZE = 1.86e6/DISK_PIECES\n",
    "\n",
    "# COMPUTE IN PARTS FOR MEMORY MANGEMENT\n",
    "for PART in range(DISK_PIECES):\n",
    "    print()\n",
    "    print('### DISK PART',PART+1)\n",
    "    \n",
    "    # MERGE IS FASTEST PROCESSING CHUNKS WITHIN CHUNKS\n",
    "    # => OUTER CHUNKS\n",
    "    for j in range(6):\n",
    "        a = j*CHUNK\n",
    "        b = min( (j+1)*CHUNK, len(files) )\n",
    "        print(f'Processing files {a} thru {b-1} in groups of {READ_CT}...')\n",
    "        \n",
    "        # => INNER CHUNKS\n",
    "        for k in range(a,b,READ_CT):\n",
    "            # READ FILE\n",
    "            df = [read_file(files[k])]\n",
    "            for i in range(1,READ_CT): \n",
    "                if k+i<b: df.append( read_file(files[k+i]) )\n",
    "            df = cudf.concat(df,ignore_index=True,axis=0)\n",
    "            df = df.sort_values(['session','ts'],ascending=[True,False])\n",
    "            # USE TAIL OF SESSION\n",
    "            df = df.reset_index(drop=True)\n",
    "            df['n'] = df.groupby('session').cumcount()\n",
    "            df = df.loc[df.n<30].drop('n',axis=1)\n",
    "            # CREATE PAIRS\n",
    "            df = df.merge(df,on='session')\n",
    "            df = df.loc[ ((df.ts_x - df.ts_y).abs()< 24 * 60 * 60) & (df.aid_x != df.aid_y) ]\n",
    "            # MEMORY MANAGEMENT COMPUTE IN PARTS\n",
    "            df = df.loc[(df.aid_x >= PART*SIZE)&(df.aid_x < (PART+1)*SIZE)]\n",
    "            # ASSIGN WEIGHTS\n",
    "            df = df[['session', 'aid_x', 'aid_y','ts_x']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n",
    "            df['wgt'] = 1 + 3*(df.ts_x - 1659304800)/(1662328791-1659304800)\n",
    "            df = df[['aid_x','aid_y','wgt']]\n",
    "            df.wgt = df.wgt.astype('float32')\n",
    "            df = df.groupby(['aid_x','aid_y']).wgt.sum()\n",
    "            # COMBINE INNER CHUNKS\n",
    "            if k==a: tmp2 = df\n",
    "            else: tmp2 = tmp2.add(df, fill_value=0)\n",
    "            print(k,', ',end='')\n",
    "        print()\n",
    "        # COMBINE OUTER CHUNKS\n",
    "        if a==0: tmp = tmp2\n",
    "        else: tmp = tmp.add(tmp2, fill_value=0)\n",
    "        del tmp2, df\n",
    "        gc.collect()\n",
    "    # CONVERT MATRIX TO DICTIONARY\n",
    "    tmp = tmp.reset_index()\n",
    "    tmp = tmp.sort_values(['aid_x','wgt'],ascending=[True,False])\n",
    "    # SAVE TOP 40\n",
    "    tmp = tmp.reset_index(drop=True)\n",
    "    tmp['n'] = tmp.groupby('aid_x').aid_y.cumcount()\n",
    "    tmp = tmp.loc[tmp.n<topn].drop('n',axis=1)\n",
    "    # SAVE PART TO DISK (convert to pandas first uses less memory)\n",
    "    tmp.to_pandas().to_parquet(f'./{output_path}/top_{topn}_clicks_v{VER}_{PART}.pqt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e03d1877-f233-4b53-882f-1cfdc3bacda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FREE MEMORY\n",
    "del data_cache, tmp\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b710209b-4735-4715-8048-3675f7ccab54",
   "metadata": {},
   "source": [
    "### Step 2 - ReRank (choose 20) using handcrafted rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a649023-f170-4e57-b80e-ceb011123f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data has shape (6928123, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>aid</th>\n",
       "      <th>ts</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12899779</td>\n",
       "      <td>59625</td>\n",
       "      <td>1661724000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12899780</td>\n",
       "      <td>1142000</td>\n",
       "      <td>1661724000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12899780</td>\n",
       "      <td>582732</td>\n",
       "      <td>1661724058</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12899780</td>\n",
       "      <td>973453</td>\n",
       "      <td>1661724109</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12899780</td>\n",
       "      <td>736515</td>\n",
       "      <td>1661724136</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    session      aid          ts  type\n",
       "0  12899779    59625  1661724000     0\n",
       "1  12899780  1142000  1661724000     0\n",
       "2  12899780   582732  1661724058     0\n",
       "3  12899780   973453  1661724109     0\n",
       "4  12899780   736515  1661724136     0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_test():    \n",
    "    dfs = []\n",
    "    for e, chunk_file in enumerate(glob.glob(test_data_path)):\n",
    "        chunk = pd.read_parquet(chunk_file)\n",
    "        chunk.ts = (chunk.ts/1000).astype('int32')\n",
    "        chunk['type'] = chunk['type'].map(type_labels).astype('int8')\n",
    "        dfs.append(chunk)\n",
    "    return pd.concat(dfs).reset_index(drop=True)\n",
    "\n",
    "test_df = load_test()\n",
    "print('Test data has shape',test_df.shape)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fca8f0e6-48de-4d85-b86d-7f5d9c1b8df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are size of our 3 co-visitation matrices:\n",
      "1837166 1837166 1168768\n",
      "CPU times: user 24.1 s, sys: 3.58 s, total: 27.7 s\n",
      "Wall time: 22.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# LOAD THREE CO-VISITATION MATRICES\n",
    "def pqt_to_dict(path):\n",
    "    return pl.read_parquet(path).groupby('aid_x').agg(pl.col('aid_y').list()).to_pandas().set_index('aid_x').aid_y.apply(list).to_dict()\n",
    "\n",
    "DISK_PIECES = 3\n",
    "# load topn clicks co visitation matrix\n",
    "topn_clicks = pqt_to_dict(f'./{output_path}/top_{topn}_clicks_v{VER}_0.pqt')\n",
    "for k in range(1, DISK_PIECES): \n",
    "    topn_clicks.update( pqt_to_dict(f'./{output_path}/top_{topn}_clicks_v{VER}_{k}.pqt') )\n",
    "\n",
    "# load topn buys co visitation matrix\n",
    "topn_buys = pqt_to_dict(f'./{output_path}/top_{topn}_carts_orders_v{VER}_0.pqt')\n",
    "for k in range(1, DISK_PIECES): \n",
    "    topn_buys.update( pqt_to_dict(f'./{output_path}/top_{topn}_carts_orders_v{VER}_{k}.pqt') )\n",
    "    \n",
    "# load topn b2b co visitation matrix\n",
    "topn_b2b = pqt_to_dict(f'./{output_path}/top_{topn}_buy2buy_v{VER}_0.pqt')\n",
    "\n",
    "# TOP CLICKS AND ORDERS IN TEST\n",
    "top_clicks = test_df.loc[test_df['type']=='clicks','aid'].value_counts().index.values[:40]\n",
    "top_orders = test_df.loc[test_df['type']=='orders','aid'].value_counts().index.values[:40]\n",
    "\n",
    "print('Here are size of our 3 co-visitation matrices:')\n",
    "print( len( topn_clicks ), len( topn_buys ), len( topn_b2b ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5d47f4c-c7b6-425d-84e0-350e5a826899",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_weight_multipliers = {0: 1, 1: 6, 2: 3}\n",
    "\n",
    "topn = 100\n",
    "\n",
    "def suggest_clicks(df):\n",
    "    # USE USER HISTORY AIDS AND TYPES\n",
    "    aids=df.aid.tolist()\n",
    "    types = df.type.tolist()\n",
    "    unique_aids = list(dict.fromkeys(aids[::-1] ))\n",
    "    # RERANK CANDIDATES USING WEIGHTS\n",
    "    if len(unique_aids)>=topn:\n",
    "        weights=np.logspace(0.1,1,len(aids),base=2, endpoint=True)-1\n",
    "        aids_temp = Counter() \n",
    "        # RERANK BASED ON REPEAT ITEMS AND TYPE OF ITEMS\n",
    "        for aid,w,t in zip(aids,weights,types): \n",
    "            aids_temp[aid] += w * type_weight_multipliers[t]\n",
    "        sorted_aids = [k for k,v in aids_temp.most_common(topn)]\n",
    "        return sorted_aids\n",
    "    # USE \"CLICKS\" CO-VISITATION MATRIX\n",
    "    aids2 = list(itertools.chain(*[topn_clicks[aid] for aid in unique_aids if aid in topn_clicks]))\n",
    "    # RERANK CANDIDATES\n",
    "    top_aids2 = [aid2 for aid2, cnt in Counter(aids2).most_common(topn) if aid2 not in unique_aids]\n",
    "    result = unique_aids + top_aids2[:topn - len(unique_aids)]\n",
    "    # USE TOP20 TEST CLICKS\n",
    "    return result + list(top_clicks)[:topn-len(result)]\n",
    "\n",
    "def suggest_buys(df):\n",
    "    # USE USER HISTORY AIDS AND TYPES\n",
    "    aids=df.aid.tolist()\n",
    "    types = df.type.tolist()\n",
    "    # UNIQUE AIDS AND UNIQUE BUYS\n",
    "    unique_aids = list(dict.fromkeys(aids[::-1] ))\n",
    "    df = df.loc[(df['type']==1)|(df['type']==2)]\n",
    "    unique_buys = list(dict.fromkeys( df.aid.tolist()[::-1] ))\n",
    "    # RERANK CANDIDATES USING WEIGHTS\n",
    "    if len(unique_aids)>=topn:\n",
    "        weights=np.logspace(0.5,1,len(aids),base=2, endpoint=True)-1\n",
    "        aids_temp = Counter() \n",
    "        # RERANK BASED ON REPEAT ITEMS AND TYPE OF ITEMS\n",
    "        for aid,w,t in zip(aids,weights,types): \n",
    "            aids_temp[aid] += w * type_weight_multipliers[t]\n",
    "        # RERANK CANDIDATES USING \"BUY2BUY\" CO-VISITATION MATRIX\n",
    "        aids3 = list(itertools.chain(*[topn_b2b[aid] for aid in unique_buys if aid in topn_b2b]))\n",
    "        for aid in aids3: aids_temp[aid] += 0.1\n",
    "        sorted_aids = [k for k,v in aids_temp.most_common(topn)]\n",
    "        return sorted_aids\n",
    "    # USE \"CART ORDER\" CO-VISITATION MATRIX\n",
    "    aids2 = list(itertools.chain(*[topn_buys[aid] for aid in unique_aids if aid in topn_buys]))\n",
    "    # USE \"BUY2BUY\" CO-VISITATION MATRIX\n",
    "    aids3 = list(itertools.chain(*[topn_b2b[aid] for aid in unique_buys if aid in topn_b2b]))\n",
    "    # RERANK CANDIDATES\n",
    "    top_aids2 = [aid2 for aid2, cnt in Counter(aids2+aids3).most_common(topn) if aid2 not in unique_aids] \n",
    "    result = unique_aids + top_aids2[:topn - len(unique_aids)]\n",
    "    # USE TOP20 TEST ORDERS\n",
    "    return result + list(top_orders)[:topn-len(result)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6f9c8c-2c0c-4e20-a5e5-380084da42a3",
   "metadata": {},
   "source": [
    "### Create submission csv\n",
    "- Inferring test data with Pandas groupby is slow. We use pandarallel to accelerate process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e54158c9-9096-4326-b22d-707dc703fd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/kaggle-env/lib/python3.9/site-packages/pandarallel/data_types/dataframe_groupby.py:18: FutureWarning: In a future version of pandas, a length 1 tuple will be returned when iterating over a groupby with a grouper equal to a list of length 1. Don't supply a list with a single grouper to avoid this warning.\n",
      "  iterator = iter(dataframe_groupby)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49.9 s, sys: 5.92 s, total: 55.8 s\n",
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pred_df_clicks = test_df.sort_values([\"session\", \"ts\"]).groupby([\"session\"]).parallel_apply(\n",
    "    lambda x: suggest_clicks(x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "693cf960-5e35-4926-8ac1-eb2ace1556b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/disk1/kaggle-env/lib/python3.9/site-packages/pandarallel/data_types/dataframe_groupby.py:18: FutureWarning: In a future version of pandas, a length 1 tuple will be returned when iterating over a groupby with a grouper equal to a list of length 1. Don't supply a list with a single grouper to avoid this warning.\n",
      "  iterator = iter(dataframe_groupby)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 2s, sys: 7.59 s, total: 1min 10s\n",
      "Wall time: 2min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pred_df_buys = test_df.sort_values([\"session\", \"ts\"]).groupby([\"session\"]).parallel_apply(\n",
    "    lambda x: suggest_buys(x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a543a36-b513-4068-9399-1c312a85c59f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_type</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12899779_clicks</td>\n",
       "      <td>59625 1253524 737445 438191 731692 1790770 942...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12899780_clicks</td>\n",
       "      <td>1142000 736515 973453 582732 1502122 487136 88...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12899781_clicks</td>\n",
       "      <td>918667 199008 194067 57315 141736 1460571 3317...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12899782_clicks</td>\n",
       "      <td>1007613 595994 1033148 834354 479970 1696036 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12899783_clicks</td>\n",
       "      <td>1817895 607638 1754419 1216820 1729553 300127 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      session_type                                             labels\n",
       "0  12899779_clicks  59625 1253524 737445 438191 731692 1790770 942...\n",
       "1  12899780_clicks  1142000 736515 973453 582732 1502122 487136 88...\n",
       "2  12899781_clicks  918667 199008 194067 57315 141736 1460571 3317...\n",
       "3  12899782_clicks  1007613 595994 1033148 834354 479970 1696036 8...\n",
       "4  12899783_clicks  1817895 607638 1754419 1216820 1729553 300127 ..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate submission dataframe\n",
    "clicks_pred_df = pd.DataFrame(pred_df_clicks.add_suffix(\"_clicks\"), columns=[\"labels\"]).reset_index()\n",
    "orders_pred_df = pd.DataFrame(pred_df_buys.add_suffix(\"_orders\"), columns=[\"labels\"]).reset_index()\n",
    "carts_pred_df = pd.DataFrame(pred_df_buys.add_suffix(\"_carts\"), columns=[\"labels\"]).reset_index()\n",
    "\n",
    "pred_df = pd.concat([clicks_pred_df, orders_pred_df, carts_pred_df])\n",
    "pred_df.columns = [\"session_type\", \"labels\"]\n",
    "pred_df[\"labels\"] = pred_df.labels.apply(lambda x: \" \".join(map(str,x)))\n",
    "# pred_df.to_csv(\"validation_preds.csv\", index=False)\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bbf881-98b2-452e-b9d6-cd802021c2f0",
   "metadata": {},
   "source": [
    "### Compute Validation Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d44a129-61da-41f3-86eb-28080bd368a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clicks recall = 0.620077423735456\n",
      "carts recall = 0.49115323635430075\n",
      "orders recall = 0.6999135022645809\n",
      "=============\n",
      "Overall Recall = 0.6293018146385844\n",
      "=============\n",
      "CPU times: user 49.6 s, sys: 2.16 s, total: 51.7 s\n",
      "Wall time: 51.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if is_validation:\n",
    "    # COMPUTE METRIC\n",
    "    score = 0\n",
    "    weights = {'clicks': 0.10, 'carts': 0.30, 'orders': 0.60}\n",
    "    for t in ['clicks','carts','orders']:\n",
    "        sub = pred_df.loc[pred_df.session_type.str.contains(t)].copy()\n",
    "        sub['session'] = sub.session_type.apply(lambda x: int(x.split('_')[0]))\n",
    "        sub.labels = sub.labels.apply(lambda x: [int(i) for i in x.split(' ')[:topn]])\n",
    "        test_labels = pd.read_parquet('./val_data/test_labels.parquet')\n",
    "        test_labels = test_labels.loc[test_labels['type']==t]\n",
    "        test_labels = test_labels.merge(sub, how='left', on=['session'])\n",
    "        test_labels['hits'] = test_labels.apply(lambda df: len(set(df.ground_truth).intersection(set(df.labels))), axis=1)\n",
    "        test_labels['gt_count'] = test_labels.ground_truth.str.len().clip(0,20)\n",
    "        recall = test_labels['hits'].sum() / test_labels['gt_count'].sum()\n",
    "        score += weights[t]*recall\n",
    "        print(f'{t} recall =',recall)\n",
    "\n",
    "    print('=============')\n",
    "    print('Overall Recall =',score)\n",
    "    print('=============')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "795b0d14-4ed5-44df-aa1e-2f1cbeee50d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.37 s, sys: 1.79 s, total: 8.15 s\n",
      "Wall time: 10.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# write candidates to local disk for stage 2 model\n",
    "if is_validation:\n",
    "    pred_df.to_parquet('./val_data/cv_candidates.pgt')\n",
    "else:\n",
    "    pred_df.to_parquet('./data/candidates.pgt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df685e0-6ee7-4387-bcef-b51bd0790408",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
