{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tracy\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "import logging\n",
    "from gensim.models import word2vec\n",
    "\n",
    "# set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] <%(processName)s> (%(threadName)s) %(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# init variables\n",
    "train_data_path = \"data/training/sentiment_analysis_trainingset.csv\"\n",
    "validate_data_path = \"data/validation/sentiment_analysis_validationset.csv\"\n",
    "test_data_path = \"data/test_a/sentiment_analysis_testa.csv\"\n",
    "test_data_predict_out_path = \"data/test_a/sentiment_analysis_testa_prediction.csv\"\n",
    "stop_world_dpath = \"data/stop_words.txt\"\n",
    "\n",
    "# loading data\n",
    "def load_data_from_csv(file_name, header=0, encoding=\"utf-8\"):\n",
    "    data_df = pd.read_csv(file_name, header=header, encoding=encoding)\n",
    "    return data_df\n",
    "\n",
    "# segment words \n",
    "def seg_words(contents):\n",
    "    contents_segs = list()\n",
    "    for content in contents:\n",
    "        segs = jieba.lcut(content)\n",
    "        content = \" \".join(segs)\n",
    "        content = content.replace(\"\\n\", \"\")\n",
    "        contents_segs.append(content)\n",
    "    return contents_segs\n",
    "\n",
    "# get stop words\n",
    "def stop_words():\n",
    "    # loading Chinese stop worlds\n",
    "    stpwrd_dic = open(stop_world_dpath, 'rb')\n",
    "    stpwrd_content = stpwrd_dic.read().decode('utf8')\n",
    "    # Decode and put into a list\n",
    "    stpwrdlst = stpwrd_content.splitlines()\n",
    "    stpwrdlst[0] = ','\n",
    "    stpwrd_dic.close()\n",
    "    return stpwrdlst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train and validation data\n",
    "train_data_df = load_data_from_csv(train_data_path)\n",
    "validate_data_df = load_data_from_csv(validate_data_path)\n",
    "test_data_df = load_data_from_csv(test_data_path)\n",
    "# loading all the comment\n",
    "content_train = train_data_df.iloc[:, 1]\n",
    "content_val = validate_data_df.iloc[:, 1]\n",
    "content_test = test_data_df.iloc[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "2018-10-01 21:10:17,691 [DEBUG] <MainProcess> (MainThread) Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\tracy\\AppData\\Local\\Temp\\jieba.cache\n",
      "2018-10-01 21:10:17,693 [DEBUG] <MainProcess> (MainThread) Loading model from cache C:\\Users\\tracy\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.582 seconds.\n",
      "2018-10-01 21:10:18,274 [DEBUG] <MainProcess> (MainThread) Loading model cost 0.582 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "2018-10-01 21:10:18,274 [DEBUG] <MainProcess> (MainThread) Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adding suggestion\n",
    "jieba.suggest_freq('大众点评', True)\n",
    "jieba.suggest_freq('大众点评网', True)\n",
    "jieba.suggest_freq('港囧', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-01 21:10:25,566 [INFO] <MainProcess> (MainThread) start seg train data\n",
      "2018-10-01 21:12:44,572 [INFO] <MainProcess> (MainThread) complete seg train data\n"
     ]
    }
   ],
   "source": [
    "# segment train comment\n",
    "logger.info(\"start seg train data\")\n",
    "content_train = seg_words(content_train)\n",
    "logger.info(\"complete seg train data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-01 21:44:04,646 [INFO] <MainProcess> (MainThread) start seg validation data\n",
      "2018-10-01 21:44:25,561 [INFO] <MainProcess> (MainThread) complete seg validation data\n"
     ]
    }
   ],
   "source": [
    "# segment validation comment\n",
    "logger.info(\"start seg validation data\")\n",
    "content_val = seg_words(content_val)\n",
    "logger.info(\"complete seg validation data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-01 21:45:28,086 [INFO] <MainProcess> (MainThread) start seg test data\n",
      "2018-10-01 21:45:48,098 [INFO] <MainProcess> (MainThread) complete seg test data\n"
     ]
    }
   ],
   "source": [
    "# segment validation comment\n",
    "logger.info(\"start seg test data\")\n",
    "content_test = seg_words(content_test)\n",
    "logger.info(\"complete seg test data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" 我 想 说 他们 家 的 优惠活动 好 持久 啊 ， 我 预售 的 时候 买 的 券 ， 前两天 心血来潮 去 吃 的 活动 还 在 继续  首先 说 下 服务 ， 因为 和 男票 开车 去 的 ， 有点 不 认路 ， 老板 很 耐心 的 在 电话 里 帮 我们 指路 ， 到 了 门店 之后 也 帮 我们 推荐 了 他们 家 做 的 比较 地道 的 伤心 凉粉 ， 说 是 厨师 是 四川 那边 来 的 。  环境 呢 比较简单 干净 ， 去 的 时候 下午 一点多 了 ， 还有 四五桌 人 在 用餐  口味 对于 我 而言 点 了 麻辣 的 口感 正 正好 ， 男票 比较 能 吃 辣 ， 相对而言 觉得 他们 家 的 麻辣 口感 麻有 了 ， 辣 还 欠缺 一点 ， 老板娘 说 考虑 到 客人 口味 不同 所以 没敢 放太多 辣椒 ， 能 吃 辣 的 朋友 可以 考虑 下单 之前 和 老板 先 说好 。 鱼 呢 我们 选 的 是 黑鱼 ， 2.9 斤 的 鱼 加上 一盆 我 以为 没有 什么 东西 实际上 东西 很多 的 锅底 ， 我们 吃 的 饱饱 的 ， 最后 以为 吃 的 差不多 了 ， 打包 一看 简直 像 没动 过 一样 ， 分量 还是 满足 的 ， 鱼 比较 新鲜 。 伤心 凉粉 很辣 ， 不过 口味 也 蛮 好吃 的 。  总的来说 ， 性价比 还是 可以 的 ， 两个 人 吃 了 大概 160 左右 ， 用 了 团购 券 的话 一百块 不到 ， 会 考虑 下次 再 来 \"'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" 终于 开 到 心心念念 的 LAB   loft 。 第一次 来 就 随便 点 也 一些 ～ 【 香辣虾 意 面 】 蛮辣 的 ， 但 其实 一般般 。 【 玛格丽特 】 进口 的 感觉 蛮 好 的 就是 喝完 后 就 点 呛 ～ 但是 朋友 不是 很 喜欢 【 一柱擎天 】 看 点评 很多 人 说 喜欢 就 点 了 ， 水蜜桃 味 ， 还 不错 挺 好喝 的 ～ 赞 【 海鲜 饭 】 想 吃饭 但 这店 的 饭类 只有 两种 ， 就 点 了 这个 。 一般般 吧 量 有点 少 ～ 性价比 不高 。 【 奶油 酒 】 这 两杯 是 送 的 ， 应该 是 圣诞 搞 活动 吧 买 半 打 送 半 打 ， 80 块 半 打 ～ 这个 真的 很 好喝 ～ 但 我 和 朋友 就 两个 人想 分别 尝尝 其它 东西 就 没有 点 了 ～ 这个 真心 推荐 ！ \"'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_test[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features from corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-01 22:08:16,867 [INFO] <MainProcess> (MainThread) start train feature extraction\n",
      "2018-10-01 22:12:00,902 [INFO] <MainProcess> (MainThread) end train feature extraction\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "logger.info(\"start train feature extraction\")\n",
    "vector = TfidfVectorizer(analyzer='word', ngram_range=(1, 5), min_df=5, norm='l2', stop_words=stop_words())\n",
    "x_train = vector.fit_transform(content_train)\n",
    "logger.info(\"end train feature extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-01 22:32:00,126 [INFO] <MainProcess> (MainThread) start train location_traffic_convenience model\n",
      "2018-10-01 22:32:00,307 [INFO] <MainProcess> (MainThread) complete train location_traffic_convenience model\n",
      "2018-10-01 22:32:00,315 [INFO] <MainProcess> (MainThread) start train location_distance_from_business_district model\n",
      "2018-10-01 22:32:00,484 [INFO] <MainProcess> (MainThread) complete train location_distance_from_business_district model\n",
      "2018-10-01 22:32:00,492 [INFO] <MainProcess> (MainThread) start train location_easy_to_find model\n",
      "2018-10-01 22:32:00,658 [INFO] <MainProcess> (MainThread) complete train location_easy_to_find model\n",
      "2018-10-01 22:32:00,666 [INFO] <MainProcess> (MainThread) start train service_wait_time model\n",
      "2018-10-01 22:32:00,836 [INFO] <MainProcess> (MainThread) complete train service_wait_time model\n",
      "2018-10-01 22:32:00,843 [INFO] <MainProcess> (MainThread) start train service_waiters_attitude model\n",
      "2018-10-01 22:32:01,009 [INFO] <MainProcess> (MainThread) complete train service_waiters_attitude model\n",
      "2018-10-01 22:32:01,017 [INFO] <MainProcess> (MainThread) start train service_parking_convenience model\n",
      "2018-10-01 22:32:01,168 [INFO] <MainProcess> (MainThread) complete train service_parking_convenience model\n",
      "2018-10-01 22:32:01,180 [INFO] <MainProcess> (MainThread) start train service_serving_speed model\n",
      "2018-10-01 22:32:01,335 [INFO] <MainProcess> (MainThread) complete train service_serving_speed model\n",
      "2018-10-01 22:32:01,341 [INFO] <MainProcess> (MainThread) start train price_level model\n",
      "2018-10-01 22:32:01,481 [INFO] <MainProcess> (MainThread) complete train price_level model\n",
      "2018-10-01 22:32:01,506 [INFO] <MainProcess> (MainThread) start train price_cost_effective model\n",
      "2018-10-01 22:32:01,646 [INFO] <MainProcess> (MainThread) complete train price_cost_effective model\n",
      "2018-10-01 22:32:01,669 [INFO] <MainProcess> (MainThread) start train price_discount model\n",
      "2018-10-01 22:32:01,812 [INFO] <MainProcess> (MainThread) complete train price_discount model\n",
      "2018-10-01 22:32:01,834 [INFO] <MainProcess> (MainThread) start train environment_decoration model\n",
      "2018-10-01 22:32:01,980 [INFO] <MainProcess> (MainThread) complete train environment_decoration model\n",
      "2018-10-01 22:32:01,992 [INFO] <MainProcess> (MainThread) start train environment_noise model\n",
      "2018-10-01 22:32:02,145 [INFO] <MainProcess> (MainThread) complete train environment_noise model\n",
      "2018-10-01 22:32:02,154 [INFO] <MainProcess> (MainThread) start train environment_space model\n",
      "2018-10-01 22:32:02,297 [INFO] <MainProcess> (MainThread) complete train environment_space model\n",
      "2018-10-01 22:32:02,312 [INFO] <MainProcess> (MainThread) start train environment_cleaness model\n",
      "2018-10-01 22:32:02,463 [INFO] <MainProcess> (MainThread) complete train environment_cleaness model\n",
      "2018-10-01 22:32:02,473 [INFO] <MainProcess> (MainThread) start train dish_portion model\n",
      "2018-10-01 22:32:02,645 [INFO] <MainProcess> (MainThread) complete train dish_portion model\n",
      "2018-10-01 22:32:02,648 [INFO] <MainProcess> (MainThread) start train dish_taste model\n",
      "2018-10-01 22:32:02,803 [INFO] <MainProcess> (MainThread) complete train dish_taste model\n",
      "2018-10-01 22:32:02,810 [INFO] <MainProcess> (MainThread) start train dish_look model\n",
      "2018-10-01 22:32:02,947 [INFO] <MainProcess> (MainThread) complete train dish_look model\n",
      "2018-10-01 22:32:02,962 [INFO] <MainProcess> (MainThread) start train dish_recommendation model\n",
      "2018-10-01 22:32:03,118 [INFO] <MainProcess> (MainThread) complete train dish_recommendation model\n",
      "2018-10-01 22:32:03,118 [INFO] <MainProcess> (MainThread) start train others_overall_experience model\n",
      "2018-10-01 22:32:03,261 [INFO] <MainProcess> (MainThread) complete train others_overall_experience model\n",
      "2018-10-01 22:32:03,261 [INFO] <MainProcess> (MainThread) start train others_willing_to_consume_again model\n",
      "2018-10-01 22:32:03,413 [INFO] <MainProcess> (MainThread) complete train others_willing_to_consume_again model\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "columns = train_data_df.columns.values.tolist()\n",
    "classifier_dict = dict()\n",
    "for column in columns[2:]:\n",
    "    train_data_df[column] = train_data_df[column].astype(int)\n",
    "    y_train = train_data_df[column]\n",
    "    clf = MultinomialNB(alpha=.1)\n",
    "    logger.info(\"start train %s model\" % column)\n",
    "    clf.fit(x_train, y_train)\n",
    "    logger.info(\"complete train %s model\" % column)\n",
    "    classifier_dict[column] = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tracy\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dish_look': 0.2181846019665296,\n",
       " 'dish_portion': 0.2092170731883179,\n",
       " 'dish_recommendation': 0.23921995096490833,\n",
       " 'dish_taste': 0.24548953352668715,\n",
       " 'environment_cleaness': 0.2232204273500469,\n",
       " 'environment_decoration': 0.24974349344032257,\n",
       " 'environment_noise': 0.22577399571141182,\n",
       " 'environment_space': 0.21780979190199287,\n",
       " 'location_distance_from_business_district': 0.22643503535773532,\n",
       " 'location_easy_to_find': 0.2204911963591554,\n",
       " 'location_traffic_convenience': 0.2445325322774234,\n",
       " 'others_overall_experience': 0.24535768940567906,\n",
       " 'others_willing_to_consume_again': 0.20392012462302886,\n",
       " 'price_cost_effective': 0.22494195060084002,\n",
       " 'price_discount': 0.2037467637575387,\n",
       " 'price_level': 0.22783173959502945,\n",
       " 'service_parking_convenience': 0.24269149693205386,\n",
       " 'service_serving_speed': 0.23214885014572625,\n",
       " 'service_wait_time': 0.23520169077137298,\n",
       " 'service_waiters_attitude': 0.28248834433919284}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validation\n",
    "x_val = vector.transform(content_val)\n",
    "f1_score_dict = dict()\n",
    "for column in columns[2:]:\n",
    "    validate_data_df[column] = validate_data_df[column].astype(int)\n",
    "    y_val = validate_data_df[column]\n",
    "    clf = classifier_dict[column]\n",
    "    pred = clf.predict(x_val)\n",
    "    f1_score_dict[column] = f1_score(pred, y_val, average='macro')\n",
    "    \n",
    "f1_score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(content_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
